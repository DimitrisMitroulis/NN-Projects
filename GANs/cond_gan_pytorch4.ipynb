{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install lpips"
      ],
      "metadata": {
        "id": "POQbrwGa19Ji",
        "outputId": "e4b2c14e-516a-41a9-9709-a56c6b0b8295",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "POQbrwGa19Ji",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting lpips\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.9/dist-packages (from lpips) (4.65.0)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from lpips) (1.10.1)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from lpips) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.9/dist-packages (from lpips) (1.22.4)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from lpips) (0.14.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.2.1->lpips) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.2.1->lpips) (8.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.2.1->lpips) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.2.1->lpips) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.2.1->lpips) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.2.1->lpips) (2022.12.7)\n",
            "Installing collected packages: lpips\n",
            "Successfully installed lpips-0.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "a493c415",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a493c415",
        "outputId": "32785770-994c-4b04-e0e8-d9e5780dc116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:cpu\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Created on Tue Jan 17 13:24:23 2023\n",
        "\n",
        "@author: DIMITRIS\n",
        "\n",
        "Description: Conditional GAN Network, for eaducational purposed\n",
        "Status: Working, needs some fine-tuning\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# %% Import and stuff\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import torchvision.utils as vutils\n",
        "from  torch.utils import data\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "from scipy import linalg\n",
        "import lpips\n",
        "\n",
        "\n",
        "NUM_EPOCHS = 150\n",
        "LR = 0.0002\n",
        "LATENT_DIM = 100\n",
        "IMG_SIZE = 28\n",
        "CHANNELS = 1\n",
        "B1 = 0.5\n",
        "B2 = 0.999\n",
        "\n",
        "\n",
        "GEN_STATE_DICT = \"gen_state_dict\"\n",
        "DISC_STATE_DICT = \"disc_state_dict\"\n",
        "GEN_OPTIMIZER = \"gen_optimizer\"\n",
        "DISC_OPTIMIZER = \"disc_optimizer\"\n",
        "G_LOSSES = \"g_losses\"\n",
        "D_LOSSES = \"d_losses\"\n",
        "\n",
        "\n",
        "\n",
        "SHUFFLE = True\n",
        "PIN_MEMORY = True\n",
        "NUM_WORKERS = 0\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "specific_latent = torch.tensor([[0.7628, 0.1779, 0.3978, 0.3606, 0.6387,\n",
        "         0.3044, 0.8340, 0.3884, 0.9313, 0.5635, 0.1994, 0.6934, 0.5326,\n",
        "         0.3676, 0.5342, 0.9480, 0.4120, 0.5845, 0.4035, 0.5298, 0.0177,\n",
        "         0.5605, 0.6453, 0.9576, 0.7153, 0.1923, 0.8122, 0.0937, 0.5744,\n",
        "         0.5951, 0.8890, 0.4838, 0.5707, 0.6760, 0.3738, 0.2796, 0.1549,\n",
        "         0.8220, 0.2800, 0.4051, 0.2553, 0.1831, 0.0046, 0.9021, 0.0264,\n",
        "         0.2327, 0.8261, 0.0534, 0.1582, 0.4087, 0.9047, 0.1409, 0.6864,\n",
        "         0.1439, 0.3432, 0.1072, 0.5907, 0.6756, 0.6942, 0.6814, 0.3368,\n",
        "         0.4138, 0.8030, 0.7024, 0.3309, 0.7288, 0.2193, 0.1954, 0.9948,\n",
        "         0.1201, 0.9483, 0.7407, 0.4849, 0.6500, 0.8649, 0.7405, 0.4725,\n",
        "         0.5373, 0.6541, 0.5444, 0.7425, 0.8940, 0.3580, 0.3905, 0.8924,\n",
        "         0.2995, 0.3726, 0.5399, 0.3057, 0.3380, 0.8313, 0.1137, 0.0120,\n",
        "         0.7714, 0.2561, 0.2569, 0.2994, 0.7648, 0.2413, 0.6101\n",
        "        ]])\n",
        "\n",
        "\n",
        "img_shape = (CHANNELS, IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Device:{}'.format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "56f4e4cf",
      "metadata": {
        "id": "56f4e4cf"
      },
      "outputs": [],
      "source": [
        "# %% helper funcitons\n",
        "\n",
        "\n",
        "def save_checkpoint(state, filename):\n",
        "    print(\"=> Saving chekpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint):\n",
        "    generator.load_state_dict(checkpoint[GEN_STATE_DICT])\n",
        "    optimizer_G.load_state_dict(checkpoint[GEN_OPTIMIZER])\n",
        "    discriminator.load_state_dict(checkpoint[DISC_STATE_DICT])\n",
        "    optimizer_D.load_state_dict(checkpoint[DISC_OPTIMIZER])\n",
        "    G_losses = checkpoint[G_LOSSES]\n",
        "    D_losses = checkpoint[D_LOSSES]\n",
        "    \n",
        "\n",
        "\n",
        "# takes input tensor and return a tensor of same size but every element has different value\n",
        "def build_fake_labels(old_list):\n",
        "  \n",
        "    new_list = []\n",
        "\n",
        "    for i, x in enumerate(old_list):\n",
        "\n",
        "        if (i % 10) != x:\n",
        "            new_list.append(i % 10)\n",
        "        else:\n",
        "            new_list.append((x.item()+1) % 10)\n",
        "\n",
        "    return torch.tensor(new_list, dtype=torch.int64).to(device)\n",
        "\n",
        "\n",
        "def add_noise(inputs, variance):\n",
        "    noise = torch.randn_like(inputs)\n",
        "    return inputs + variance*noise\n",
        "\n",
        "def gen_image(caption=-1,randomLatent=True):\n",
        "    generator.to('cpu')\n",
        "    discriminator.to('cpu')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for image,_ in train_loader:\n",
        "            f, axarr = plt.subplots(1)\n",
        "            \n",
        "            if randomLatent:\n",
        "                latent = torch.rand_like(torch.Tensor(1,100))\n",
        "            else:\n",
        "                latent = specific_latent\n",
        "                \n",
        "            if caption == -1:\n",
        "                caption = random.randint(0, 9)\n",
        "            \n",
        "            caption = torch.tensor(caption, dtype=torch.int64)\n",
        "            fake_image = generator(latent,caption)  \n",
        "           \n",
        "            \n",
        "            #axarr.imshow(add_noise(image[0][0],0.5))    \n",
        "            axarr.imshow(fake_image[0][0])   \n",
        "            print(\"Supposed to be %d\" %caption.item())\n",
        "    \n",
        "            break\n",
        "        \n",
        "def discriminate_image(caption=-1,genOrReal=0):#random.randint(0, 1)):\n",
        "    generator.to('cpu')\n",
        "    discriminator.to('cpu')\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for  i, (imgs, labels) in enumerate(example_loader):\n",
        "            f, axarr = plt.subplots(1)\n",
        "            \n",
        "            fake_labels = build_fake_labels(labels.to(device))\n",
        "            labels = labels.to('cpu')\n",
        "            z = Variable(Tensor(np.random.normal(0, 1, (1,LATENT_DIM)))).cpu()\n",
        "            if caption == -1:\n",
        "                caption = random.randint(0, 9)\n",
        "            caption = torch.tensor(caption, dtype=torch.int64)\n",
        "            \n",
        "            \n",
        "            #feed discriminator fake image, expect \"0\" output\n",
        "            if genOrReal == 0:\n",
        "                fake_image = generator(z,caption)\n",
        "                axarr.imshow(fake_image[0].reshape(-1, 28, 28)[0])\n",
        "                pred = discriminator(fake_image,caption).detach()\n",
        "                print(\"Discriminator Prediction: {},Should be: {}, label = {}\".format(pred,\"0\",caption))\n",
        "            #feed discriminator real image, expect \"1\" output\n",
        "            else:\n",
        "                fake_image = generator(z,labels[0])\n",
        "                axarr.imshow(imgs[0].reshape(-1, 28, 28)[0])\n",
        "                pred = discriminator(imgs.detach(),labels[0].detach()).detach()\n",
        "                print(\"Discriminator Prediction: {},Should be: {}, label= {}\".format(pred,\"1\",labels[0]+1))\n",
        "            \n",
        "    \n",
        "            break        \n",
        "\n",
        "# Check if a tensor image is normalized in [-1,1]\n",
        "# returns true if it s within [-1,1]\n",
        "def is_normalized(image):\n",
        "  \n",
        "  tensor_image_normalized = image[0]  # your tensor image here\n",
        "\n",
        "  min_value = tensor_image_normalized.min().item()\n",
        "  max_value = tensor_image_normalized.max().item()\n",
        "\n",
        "  \n",
        "  return (min_value >= -1.0 and max_value <= 1.0)\n",
        "\n",
        "\n",
        "def colorize(image):\n",
        "  return torch.cat([image, image, image], dim=1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "e8e59f16",
      "metadata": {
        "id": "e8e59f16"
      },
      "outputs": [],
      "source": [
        "# %%train data\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "    ])\n",
        "\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"~/torch_datasets\", train=True, transform=transform, download=True\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"~/torch_datasets\", train=False, transform=transform, download=True\n",
        ")\n",
        "\n",
        "train_loader = data.DataLoader(\n",
        "                                train_dataset,\n",
        "                                batch_size=BATCH_SIZE,\n",
        "                                shuffle=False,\n",
        "                                num_workers=NUM_WORKERS,\n",
        "                                pin_memory=False\n",
        "                                )\n",
        "\n",
        "test_loader = data.DataLoader(\n",
        "                                test_dataset,\n",
        "                                batch_size=32,\n",
        "                                shuffle=True,\n",
        "                                num_workers=0\n",
        "                                )\n",
        "\n",
        "example_loader = data.DataLoader(\n",
        "                                train_dataset,\n",
        "                                batch_size=1,\n",
        "                                shuffle=True,\n",
        "                                num_workers=0,\n",
        "                                drop_last=True,\n",
        "                                )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "97624d9f",
      "metadata": {
        "id": "97624d9f"
      },
      "outputs": [],
      "source": [
        "# %% Detective: fake or no fake -> 1 output [0, 1]\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(2, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 1)\n",
        "\n",
        "        self.emb = nn.Embedding(10, 50)\n",
        "        self.emb_fc = nn.Linear(50, 784)\n",
        "\n",
        "        self.nconv1 = nn.Conv2d(2, 64, kernel_size=5)\n",
        "        self.nconv2 = nn.Conv2d(64, 128, kernel_size=5)\n",
        "        self.pool = nn.AvgPool2d(kernel_size=3)\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=2)\n",
        "        self.nfc1 = nn.Linear(1152, 164)\n",
        "        self.nfc2 = nn.Linear(164, 1)\n",
        "\n",
        "    # oldWay flag to select between 2 train methods, not sure which is best yet\n",
        "    def forward(self, x, c, oldWay=False):\n",
        "\n",
        "        c = self.emb(c)\n",
        "        c = self.emb_fc(c)\n",
        "        c = c.view(-1, 1, 28, 28)\n",
        "        x = torch.cat((c, x), 1)  # concat image[1,28,28] with text [1,28,28]\n",
        "\n",
        "        x = F.leaky_relu(self.nconv1(x))\n",
        "        x = F.leaky_relu(self.nconv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(-1, 1152)\n",
        "        x = F.leaky_relu(self.nfc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.nfc2(x)\n",
        "\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# %% Generate Fake Data: output like real data [1, 28, 28] and values -1, 1\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(LATENT_DIM, 7*7*63)  # [n,100]->[n,3087]\n",
        "        self.ct1 = nn.ConvTranspose2d(64, 32, 4, stride=2)  # [n, 64, 16, 16] [32,..,..]\n",
        "        self.ct2 = nn.ConvTranspose2d(32, 16, 4, stride=2)  # [n, 32, , ]->[n, 16, 34, 34]\n",
        "        self.conv = nn.Conv2d(16, 1, kernel_size=7)  # [n, 16, 34, 34]-> [n, 1, 28, 28]\n",
        "        \n",
        "        self.emb = nn.Embedding(10, 50) \n",
        "        self.label_lin = nn.Linear(50, 49)\n",
        "        self.conv_x_c = nn.ConvTranspose2d(65, 64, 4, stride=2)  # upsample [65,7,7] -> [64,14,14]\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        # Pass latent space input into linear layer and reshape\n",
        "        x = self.lin1(x)  # (n,100) -> (n,3187)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = x.view(-1, 63, 7, 7)  # (n,3187) -> (63,7,7)\n",
        "        \n",
        "        #Encode label\n",
        "        c = self.emb(c)  # (n,) -> (n,50)\n",
        "        c = self.label_lin(c)  # (n,50) -> (n,49)\n",
        "        c = c.view(-1, 1, 7, 7)  # (n,49) -> (n,1,7,7)\n",
        "        x = torch.cat((c, x), 1) # concat image[63,7,7] with text [1,7,7]\n",
        "\n",
        "        x = self.ct1(x)  # [n, 64, 16, 16] [32,34,34]\n",
        "        x = F.leaky_relu(x)\n",
        "\n",
        "        # Upsample to 34x34 (16 feature maps)\n",
        "        x = self.ct2(x)\n",
        "        x = F.leaky_relu(x)\n",
        "\n",
        "        # Convolution to 28x28 (1 feature map)\n",
        "        x = self.tanh(self.conv(x))\n",
        "        return x\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "abc07c4f",
      "metadata": {
        "id": "abc07c4f"
      },
      "outputs": [],
      "source": [
        "#%% Loss fucntion, optimizers\n",
        "loss_func = nn.BCELoss()\n",
        "d_loss_func = nn.BCELoss()\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    loss_func.cuda()\n",
        "\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=LR,betas=(B1 ,B2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=LR,betas=(B1 ,B2))\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "9e61a176",
      "metadata": {
        "id": "9e61a176"
      },
      "outputs": [],
      "source": [
        "load_checkpoint(torch.load(\"cond_gan_pytorch10.pth.tar\",map_location=(device)))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen_image()"
      ],
      "metadata": {
        "id": "fmyJkwav21hA",
        "outputId": "de84ffa1-15c5-42f3-a3ef-3be75e8e5aeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        }
      },
      "id": "fmyJkwav21hA",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supposed to be 4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "b608e244",
      "metadata": {
        "id": "b608e244"
      },
      "outputs": [],
      "source": [
        "\n",
        "fake_images = []\n",
        "real_images = []\n",
        "with torch.no_grad():\n",
        "    generator.to('cpu')\n",
        "    discriminator.to('cpu')\n",
        "    \n",
        "    for i, (imgs,_) in enumerate(train_loader):\n",
        "        real_images = imgs\n",
        "        \n",
        "        caption = random.randint(0, 9)   \n",
        "        caption = torch.tensor(caption, dtype=torch.int64)\n",
        "        latent = torch.rand_like(torch.Tensor(1,100))\n",
        "        fake_images = generator(latent,caption)\n",
        "        \n",
        "        \n",
        "        for i in range(len(imgs[:,0,0,0])):\n",
        "            \n",
        "            caption = random.randint(0, 9)   \n",
        "            caption = torch.tensor(caption, dtype=torch.int64)\n",
        "            latent = torch.rand_like(torch.Tensor(1,100))\n",
        "            \n",
        "\n",
        "            \n",
        "            fake_image = generator(latent,caption)\n",
        "            fake_images = torch.cat((fake_images, fake_image), 0)\n",
        "\n",
        "        break\n",
        "    \n",
        "    fake_images = fake_images[:BATCH_SIZE,:,:,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "48014174",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48014174",
        "outputId": "9cdac570-2bdf-4c5c-d7ac-b7fd66d5934b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.90150654\n"
          ]
        }
      ],
      "source": [
        "#%% Inception score\n",
        "from torchvision.models import inception_v3\n",
        "from scipy.stats import entropy\n",
        "\n",
        "with torch.no_grad():\n",
        "    images = fake_images\n",
        "    batch_size = BATCH_SIZE\n",
        "    resize=True\n",
        "    \n",
        "    # Load pre-trained Inception-v3 model\n",
        "    model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
        "    model.eval()\n",
        "    model.requires_grad_ = False\n",
        "\n",
        "    \n",
        "    # Prepare the images\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "    if resize:\n",
        "        images = torch.cat([images,images,images],dim=1)\n",
        "        images = transforms.Compose([\n",
        "            transforms.Resize((299, 299)),\n",
        "            transforms.Grayscale(num_output_channels=3),\n",
        "        ])(images)\n",
        "    else:\n",
        "        images = [torch.from_numpy(image.transpose(2, 0, 1)).float().div(255).unsqueeze(0) for image in images]\n",
        "    \n",
        "    # Compute the predictions\n",
        "    n_images = images.shape[0]\n",
        "    n_batches = int(np.ceil(n_images / batch_size))\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(n_batches):\n",
        "            start_idx = i * batch_size\n",
        "            end_idx = min((i + 1) * batch_size, n_images)\n",
        "            batch = images.to(device)\n",
        "            #batch = torch.cat(images[start_idx:end_idx], dim=0).to(device)\n",
        "            pred = model(batch.detach())\n",
        "            pred = F.softmax(pred, dim=1).cpu().numpy()\n",
        "            preds.append(pred)\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "    \n",
        "    # Compute the Inception Score\n",
        "    scores = []\n",
        "    for i in range(preds.shape[0]):\n",
        "        p_yx = preds[i]\n",
        "        p_y = np.expand_dims(np.mean(p_yx, axis=0), axis=0)\n",
        "        scores.append(entropy(p_yx.T, p_y.T))\n",
        "    kl_divergence = np.mean(scores)\n",
        "    entropy_y = entropy(np.mean(preds, axis=0))\n",
        "    inception_score = np.exp(kl_divergence - entropy_y)\n",
        "    print(inception_score)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FID score\n",
        "\n",
        "f_images = torch.cat([fake_images, fake_images, fake_images], dim=1)\n",
        "f_images = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "])(f_images)\n",
        "\n",
        "\n",
        "# load a pre-trained Inception-v3 model\n",
        "inception_model = inception_v3(pretrained=True, aux_logits=True,)\n",
        "inception_model.to(device)\n",
        "inception_model.eval()\n",
        "\n",
        "# compute the feature representations of the real and fake images\n",
        "real_features = []\n",
        "fake_features = []\n",
        "for batch in train_loader:\n",
        "    images, _ = batch\n",
        "    images = images.to(device)\n",
        "    with torch.no_grad():\n",
        "        images = torch.cat([images,images,images],dim=1)\n",
        "        images = transforms.Compose([\n",
        "            transforms.Resize((299, 299)),\n",
        "            transforms.Grayscale(num_output_channels=3),\n",
        "        ])(images)\n",
        "\n",
        "           \n",
        "        features = inception_model(images).view(images.size(0), -1)\n",
        "    real_features.append(features.cpu().numpy())\n",
        "    break\n",
        "with torch.no_grad():\n",
        "    features = inception_model(f_images).view(f_images.size(0), -1)\n",
        "fake_features.append(features.cpu().numpy())\n",
        "\n",
        "# calculate the mean and covariance of the feature representations\n",
        "real_features = np.concatenate(real_features, axis=0)\n",
        "fake_features = np.concatenate(fake_features, axis=0)\n",
        "mu1, sigma1 = np.mean(real_features, axis=0), np.cov(real_features, rowvar=False)\n",
        "mu2, sigma2 = np.mean(fake_features, axis=0), np.cov(fake_features, rowvar=False)\n",
        "\n",
        "# calculate the FID score\n",
        "mu_diff = mu1 - mu2\n",
        "sigma_diff_sqrt = linalg.sqrtm(sigma1 @ sigma2)\n",
        "fid_score = np.real(np.trace(sigma1 + sigma2 - 2*sigma_diff_sqrt)) + np.dot(mu_diff, mu_diff)\n",
        "print(f'FID score: {fid_score:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUpzfww27ah2",
        "outputId": "80f40b72-2b1d-4650-fa7e-21275485a2c3"
      },
      "id": "gUpzfww27ah2",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FID score: 420.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "645ba33b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "645ba33b",
        "outputId": "30a0e3b3-7aa7-4ad7-8cc5-4ccd95fa0448"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.9/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "LPIPS?  tensor([[[0.3054]]], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "#LPIPS metric \n",
        "loss_fn_alex = lpips.LPIPS(net='alex') \n",
        "\n",
        "lpip_fake = colorize(fake_images)\n",
        "lpip_real = colorize(real_images) # !! images must be rbg and within [-1,1]\n",
        "\n",
        "lpip_fake = transforms.Compose([\n",
        "    transforms.Resize((64, 64))])(lpip_fake)\n",
        "\n",
        "lpip_real = transforms.Compose([\n",
        "    transforms.Resize((64, 64))])(lpip_real)\n",
        "\n",
        "\n",
        "\n",
        "d = loss_fn_alex(lpip_fake, lpip_real)\n",
        "print(\"LPIPS? \",d[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cl7UvjhH4btt",
        "outputId": "b91b36bb-e279-4bfb-a26c-be314092b51f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cl7UvjhH4btt",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([33, 3, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}