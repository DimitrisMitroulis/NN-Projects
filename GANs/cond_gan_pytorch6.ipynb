{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install lpips"
      ],
      "metadata": {
        "id": "RRFEJ4rYk_Y3",
        "outputId": "6a084d8a-deeb-414f-d70e-bf30214e9594",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "RRFEJ4rYk_Y3",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lpips in /usr/local/lib/python3.9/dist-packages (0.1.4)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from lpips) (1.10.1)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from lpips) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.9/dist-packages (from lpips) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.9/dist-packages (from lpips) (4.65.0)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from lpips) (0.15.1+cu118)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips) (3.10.7)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips) (3.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=0.4.0->lpips) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=0.4.0->lpips) (16.0.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.2.1->lpips) (8.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.2.1->lpips) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=0.4.0->lpips) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.2.1->lpips) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.2.1->lpips) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.2.1->lpips) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.2.1->lpips) (2.0.12)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=0.4.0->lpips) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a493c415",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a493c415",
        "outputId": "beb1fae1-f8db-4be1-afe8-32ac0a26f807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:cuda\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Created on Tue Jan 17 13:24:23 2023\n",
        "\n",
        "@author: DIMITRIS\n",
        "\n",
        "Description: Conditional GAN Network, for eaducational purposed\n",
        "Status: Working, needs some fine-tuning\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# %% Import and stuff\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import torchvision.utils as vutils\n",
        "from  torch.utils import data\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "from scipy import linalg\n",
        "import lpips\n",
        "\n",
        "\n",
        "NUM_EPOCHS = 150\n",
        "LR = 0.0002\n",
        "LATENT_DIM = 100\n",
        "IMG_SIZE = 28\n",
        "CHANNELS = 1\n",
        "B1 = 0.5\n",
        "B2 = 0.999\n",
        "\n",
        "\n",
        "GEN_STATE_DICT = \"gen_state_dict\"\n",
        "DISC_STATE_DICT = \"disc_state_dict\"\n",
        "GEN_OPTIMIZER = \"gen_optimizer\"\n",
        "DISC_OPTIMIZER = \"disc_optimizer\"\n",
        "G_LOSSES = \"g_losses\"\n",
        "D_LOSSES = \"d_losses\"\n",
        "\n",
        "\n",
        "\n",
        "SHUFFLE = True\n",
        "PIN_MEMORY = True\n",
        "NUM_WORKERS = 0\n",
        "#small batch size cause inception_v3 hogs ram\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "specific_latent = torch.tensor([[0.7628, 0.1779, 0.3978, 0.3606, 0.6387,\n",
        "         0.3044, 0.8340, 0.3884, 0.9313, 0.5635, 0.1994, 0.6934, 0.5326,\n",
        "         0.3676, 0.5342, 0.9480, 0.4120, 0.5845, 0.4035, 0.5298, 0.0177,\n",
        "         0.5605, 0.6453, 0.9576, 0.7153, 0.1923, 0.8122, 0.0937, 0.5744,\n",
        "         0.5951, 0.8890, 0.4838, 0.5707, 0.6760, 0.3738, 0.2796, 0.1549,\n",
        "         0.8220, 0.2800, 0.4051, 0.2553, 0.1831, 0.0046, 0.9021, 0.0264,\n",
        "         0.2327, 0.8261, 0.0534, 0.1582, 0.4087, 0.9047, 0.1409, 0.6864,\n",
        "         0.1439, 0.3432, 0.1072, 0.5907, 0.6756, 0.6942, 0.6814, 0.3368,\n",
        "         0.4138, 0.8030, 0.7024, 0.3309, 0.7288, 0.2193, 0.1954, 0.9948,\n",
        "         0.1201, 0.9483, 0.7407, 0.4849, 0.6500, 0.8649, 0.7405, 0.4725,\n",
        "         0.5373, 0.6541, 0.5444, 0.7425, 0.8940, 0.3580, 0.3905, 0.8924,\n",
        "         0.2995, 0.3726, 0.5399, 0.3057, 0.3380, 0.8313, 0.1137, 0.0120,\n",
        "         0.7714, 0.2561, 0.2569, 0.2994, 0.7648, 0.2413, 0.6101\n",
        "        ]])\n",
        "\n",
        "\n",
        "img_shape = (CHANNELS, IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Device:{}'.format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "56f4e4cf",
      "metadata": {
        "id": "56f4e4cf"
      },
      "outputs": [],
      "source": [
        "# %% helper funcitons\n",
        "\n",
        "\n",
        "def save_checkpoint(state, filename):\n",
        "    print(\"=> Saving chekpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint):\n",
        "    generator.load_state_dict(checkpoint[GEN_STATE_DICT])\n",
        "    optimizer_G.load_state_dict(checkpoint[GEN_OPTIMIZER])\n",
        "    discriminator.load_state_dict(checkpoint[DISC_STATE_DICT])\n",
        "    optimizer_D.load_state_dict(checkpoint[DISC_OPTIMIZER])\n",
        "    G_losses = checkpoint.get(G_LOSSES, 0)\n",
        "    D_losses = checkpoint.get(D_LOSSES, 0)\n",
        "    \n",
        "\n",
        "\n",
        "# takes input tensor and return a tensor of same size but every element has different value\n",
        "def build_fake_labels(old_list):\n",
        "  \n",
        "    new_list = []\n",
        "\n",
        "    for i, x in enumerate(old_list):\n",
        "\n",
        "        if (i % 10) != x:\n",
        "            new_list.append(i % 10)\n",
        "        else:\n",
        "            new_list.append((x.item()+1) % 10)\n",
        "\n",
        "    return torch.tensor(new_list, dtype=torch.int64).to(device)\n",
        "\n",
        "\n",
        "def add_noise(inputs, variance):\n",
        "    noise = torch.randn_like(inputs)\n",
        "    return inputs + variance*noise\n",
        "\n",
        "def gen_image(caption=-1,randomLatent=True):\n",
        "    generator.to('cpu')\n",
        "    discriminator.to('cpu')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for image,_ in train_loader:\n",
        "            f, axarr = plt.subplots(1)\n",
        "            \n",
        "            if randomLatent:\n",
        "                latent = torch.rand_like(torch.Tensor(1,100))\n",
        "            else:\n",
        "                latent = specific_latent\n",
        "                \n",
        "            if caption == -1:\n",
        "                caption = random.randint(0, 9)\n",
        "            \n",
        "            caption = torch.tensor(caption, dtype=torch.int64)\n",
        "            fake_image = generator(latent,caption)  \n",
        "           \n",
        "            \n",
        "            #axarr.imshow(add_noise(image[0][0],0.5))    \n",
        "            axarr.imshow(fake_image[0][0])   \n",
        "            print(\"Supposed to be %d\" %caption.item())\n",
        "    \n",
        "            break\n",
        "        \n",
        "def discriminate_image(caption=-1,genOrReal=0):#random.randint(0, 1)):\n",
        "    generator.to('cpu')\n",
        "    discriminator.to('cpu')\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for  i, (imgs, labels) in enumerate(example_loader):\n",
        "            f, axarr = plt.subplots(1)\n",
        "            \n",
        "            fake_labels = build_fake_labels(labels.to(device))\n",
        "            labels = labels.to('cpu')\n",
        "            z = Variable(Tensor(np.random.normal(0, 1, (1,LATENT_DIM)))).cpu()\n",
        "            if caption == -1:\n",
        "                caption = random.randint(0, 9)\n",
        "            caption = torch.tensor(caption, dtype=torch.int64)\n",
        "            \n",
        "            \n",
        "            #feed discriminator fake image, expect \"0\" output\n",
        "            if genOrReal == 0:\n",
        "                fake_image = generator(z,caption)\n",
        "                axarr.imshow(fake_image[0].reshape(-1, 28, 28)[0])\n",
        "                pred = discriminator(fake_image,caption).detach()\n",
        "                print(\"Discriminator Prediction: {},Should be: {}, label = {}\".format(pred,\"0\",caption))\n",
        "            #feed discriminator real image, expect \"1\" output\n",
        "            else:\n",
        "                fake_image = generator(z,labels[0])\n",
        "                axarr.imshow(imgs[0].reshape(-1, 28, 28)[0])\n",
        "                pred = discriminator(imgs.detach(),labels[0].detach()).detach()\n",
        "                print(\"Discriminator Prediction: {},Should be: {}, label= {}\".format(pred,\"1\",labels[0]+1))\n",
        "            \n",
        "    \n",
        "            break        \n",
        "\n",
        "# Check if a tensor image is normalized in [-1,1]\n",
        "# returns true if it s within [-1,1]\n",
        "def is_normalized(image):\n",
        "  \n",
        "  tensor_image_normalized = image[0]  # your tensor image here\n",
        "\n",
        "  min_value = tensor_image_normalized.min().item()\n",
        "  max_value = tensor_image_normalized.max().item()\n",
        "\n",
        "  \n",
        "  return (min_value >= -1.0 and max_value <= 1.0)\n",
        "\n",
        "\n",
        "def colorize(image):\n",
        "  return torch.cat([image, image, image], dim=1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e8e59f16",
      "metadata": {
        "id": "e8e59f16"
      },
      "outputs": [],
      "source": [
        "# %%train data\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "    ])\n",
        "\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"~/torch_datasets\", train=True, transform=transform, download=True\n",
        ")\n",
        "\n",
        "\n",
        "train_loader = data.DataLoader(\n",
        "                                train_dataset,\n",
        "                                batch_size=BATCH_SIZE,\n",
        "                                shuffle=False,\n",
        "                                num_workers=NUM_WORKERS,\n",
        "                                pin_memory=False\n",
        "                                )\n",
        "\n",
        "test_loader = data.DataLoader(\n",
        "                                train_dataset,\n",
        "                                batch_size=60000,\n",
        "                                shuffle=False,\n",
        "                                num_workers=NUM_WORKERS,\n",
        "                                pin_memory=False\n",
        "                                )\n",
        "\n",
        "example_loader = data.DataLoader(\n",
        "                                train_dataset,\n",
        "                                batch_size=1,\n",
        "                                shuffle=True,\n",
        "                                num_workers=0,\n",
        "                                drop_last=True,\n",
        "                                )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "97624d9f",
      "metadata": {
        "id": "97624d9f"
      },
      "outputs": [],
      "source": [
        "# %% Detective: fake or no fake -> 1 output [0, 1]\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(2, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 1)\n",
        "\n",
        "        self.emb = nn.Embedding(10, 50)\n",
        "        self.emb_fc = nn.Linear(50, 784)\n",
        "\n",
        "        self.nconv1 = nn.Conv2d(2, 64, kernel_size=5)\n",
        "        self.nconv2 = nn.Conv2d(64, 128, kernel_size=5)\n",
        "        self.pool = nn.AvgPool2d(kernel_size=3)\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=2)\n",
        "        self.nfc1 = nn.Linear(1152, 164)\n",
        "        self.nfc2 = nn.Linear(164, 1)\n",
        "\n",
        "    # oldWay flag to select between 2 train methods, not sure which is best yet\n",
        "    def forward(self, x, c, oldWay=False):\n",
        "\n",
        "        c = self.emb(c)\n",
        "        c = self.emb_fc(c)\n",
        "        c = c.view(-1, 1, 28, 28)\n",
        "        x = torch.cat((c, x), 1)  # concat image[1,28,28] with text [1,28,28]\n",
        "\n",
        "        x = F.leaky_relu(self.nconv1(x))\n",
        "        x = F.leaky_relu(self.nconv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(-1, 1152)\n",
        "        x = F.leaky_relu(self.nfc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.nfc2(x)\n",
        "\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# %% Generate Fake Data: output like real data [1, 28, 28] and values -1, 1\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(LATENT_DIM, 7*7*63)  # [n,100]->[n,3087]\n",
        "        self.ct1 = nn.ConvTranspose2d(64, 32, 4, stride=2)  # [n, 64, 16, 16] [32,..,..]\n",
        "        self.ct2 = nn.ConvTranspose2d(32, 16, 4, stride=2)  # [n, 32, , ]->[n, 16, 34, 34]\n",
        "        self.conv = nn.Conv2d(16, 1, kernel_size=7)  # [n, 16, 34, 34]-> [n, 1, 28, 28]\n",
        "        \n",
        "        self.emb = nn.Embedding(10, 50) \n",
        "        self.label_lin = nn.Linear(50, 49)\n",
        "        self.conv_x_c = nn.ConvTranspose2d(65, 64, 4, stride=2)  # upsample [65,7,7] -> [64,14,14]\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        # Pass latent space input into linear layer and reshape\n",
        "        x = self.lin1(x)  # (n,100) -> (n,3187)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = x.view(-1, 63, 7, 7)  # (n,3187) -> (63,7,7)\n",
        "        \n",
        "        #Encode label\n",
        "        c = self.emb(c)  # (n,) -> (n,50)\n",
        "        c = self.label_lin(c)  # (n,50) -> (n,49)\n",
        "        c = c.view(-1, 1, 7, 7)  # (n,49) -> (n,1,7,7)\n",
        "        x = torch.cat((c, x), 1) # concat image[63,7,7] with text [1,7,7]\n",
        "\n",
        "        x = self.ct1(x)  # [n, 64, 16, 16] [32,34,34]\n",
        "        x = F.leaky_relu(x)\n",
        "\n",
        "        # Upsample to 34x34 (16 feature maps)\n",
        "        x = self.ct2(x)\n",
        "        x = F.leaky_relu(x)\n",
        "\n",
        "        # Convolution to 28x28 (1 feature map)\n",
        "        x = self.tanh(self.conv(x))\n",
        "        return x\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "abc07c4f",
      "metadata": {
        "id": "abc07c4f"
      },
      "outputs": [],
      "source": [
        "#%% Loss fucntion, optimizers\n",
        "loss_func = nn.BCELoss()\n",
        "d_loss_func = nn.BCELoss()\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    loss_func.cuda()\n",
        "\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=LR,betas=(B1 ,B2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=LR,betas=(B1 ,B2))\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9e61a176",
      "metadata": {
        "id": "9e61a176"
      },
      "outputs": [],
      "source": [
        "#if you're running this on colab, download corresponding chackpoint file and upload it to runtime\n",
        "load_checkpoint(torch.load(\"cond_gan_pytorch_good.pth.tar\",map_location=(device)))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test image generation\n",
        "gen_image()"
      ],
      "metadata": {
        "id": "fmyJkwav21hA",
        "outputId": "40f799d1-17ec-4b56-e66f-891069b9f618",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        }
      },
      "id": "fmyJkwav21hA",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supposed to be 4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcsUlEQVR4nO3df3BV9f3n8dfNrytgcjHE5CYSMKBCFYgjhZivSrHkS4gzfEFZ11+7C46DKw2OSK1OugradjYtziijQ2F3x0KdCv6YFVj9WroaTBhroEuUpUw1X8LGAl9IUFzuDcGEQD77B+ttryTo53LDOwnPx8yZyT3nvO958+HAKyfn5HMDzjknAAAusBTrBgAAFycCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACbSrBv4pu7ubh06dEiZmZkKBALW7QAAPDnn1NbWpoKCAqWk9H6d0+8C6NChQyosLLRuAwBwng4cOKCRI0f2ur3fBVBmZqYk6WbdpjSlG3cDAPB1Sl36QO/E/j/vTZ8F0KpVq/Tss8+qpaVFxcXFevHFFzV16tRvrfv6x25pSldagAACgAHn/88w+m23UfrkIYTXXntNS5cu1fLly/XRRx+puLhY5eXlOnLkSF8cDgAwAPVJAD333HNauHCh7r//fl177bVas2aNhg4dqt/85jd9cTgAwACU9AA6efKkGhoaVFZW9reDpKSorKxM9fX1Z+3f2dmpaDQatwAABr+kB9AXX3yh06dPKy8vL259Xl6eWlpaztq/urpaoVAotvAEHABcHMx/EbWqqkqRSCS2HDhwwLolAMAFkPSn4HJycpSamqrW1ta49a2trQqHw2ftHwwGFQwGk90GAKCfS/oVUEZGhiZPnqyamprYuu7ubtXU1Ki0tDTZhwMADFB98ntAS5cu1fz58/X9739fU6dO1cqVK9Xe3q7777+/Lw4HABiA+iSA7rrrLn3++edatmyZWlpadP3112vLli1nPZgAALh4BZxzzrqJvxeNRhUKhTRdc5gJAQAGoFOuS7XarEgkoqysrF73M38KDgBwcSKAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgIs26AaBfSUn1L8lI967p7ujwrsEZn/+Pcd41ayb+LqFjPfLTh71rsjZsT+hYFyOugAAAJgggAICJpAfQ008/rUAgELeMHz8+2YcBAAxwfXIP6LrrrtN77733t4OkcasJABCvT5IhLS1N4XC4L94aADBI9Mk9oL1796qgoEBjxozRfffdp/379/e6b2dnp6LRaNwCABj8kh5AJSUlWrdunbZs2aLVq1erublZt9xyi9ra2nrcv7q6WqFQKLYUFhYmuyUAQD+U9ACqqKjQnXfeqUmTJqm8vFzvvPOOjh07ptdff73H/auqqhSJRGLLgQMHkt0SAKAf6vOnA4YPH65rrrlGTU1NPW4PBoMKBoN93QYAoJ/p898DOn78uPbt26f8/Py+PhQAYABJegA99thjqqur02effaYPP/xQt99+u1JTU3XPPfck+1AAgAEs6T+CO3jwoO655x4dPXpUl19+uW6++WZt375dl19+ebIPBQAYwJIeQK+++mqy3xIXudScEd41l7yZ2MV90bCj3jVbX7rBuyb31x961wxGaUWjvWv++fqXvGuGJjDJrCSdyPU/j7ISOtLFibngAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmOjzD6QDzteny67yrtkz5sWEjpUe8J+08priyd41ud4Vg9OXN/p/Tlhmiv9/W4dOn/aukaQr/ufn3jWJHenixBUQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEs2Gj30s77v99UiKzWkvSxye7vWuuXncyoWMNNqnDQ941E5b82bsmJYHvm1+PXO9dI0nuYEtCdfhuuAICAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABggslIcWEFAt4l3RnOv0b+k4pK0t6TYe+a9ENfetec8q7o/w4svM67ZvPIF71rTrjT3jUv/a+bvWsk6ZrjDQnV4bvhCggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJJiPFBZWWl+tdc+2Uz5LfSC+aOvK8a1z0eB90YiyBSWO7prR516TI/zirvrzeu2Z85R7vGknqdv4T4eK74woIAGCCAAIAmPAOoG3btmn27NkqKChQIBDQpk2b4rY757Rs2TLl5+dryJAhKisr0969e5PVLwBgkPAOoPb2dhUXF2vVqlU9bl+xYoVeeOEFrVmzRjt27NCwYcNUXl6ujo6O824WADB4eD+EUFFRoYqKih63Oee0cuVKPfnkk5ozZ44k6eWXX1ZeXp42bdqku++++/y6BQAMGkm9B9Tc3KyWlhaVlZXF1oVCIZWUlKi+vr7Hms7OTkWj0bgFADD4JTWAWlpaJEl5efGPsubl5cW2fVN1dbVCoVBsKSwsTGZLAIB+yvwpuKqqKkUikdhy4MAB65YAABdAUgMoHA5LklpbW+PWt7a2xrZ9UzAYVFZWVtwCABj8khpARUVFCofDqqmpia2LRqPasWOHSktLk3koAMAA5/0U3PHjx9XU1BR73dzcrF27dik7O1ujRo3SkiVL9Itf/EJXX321ioqK9NRTT6mgoEBz585NZt8AgAHOO4B27typW2+9NfZ66dKlkqT58+dr3bp1evzxx9Xe3q4HH3xQx44d080336wtW7bokksuSV7XAIABL+Bc/5ptLxqNKhQKabrmKC2Qbt0OziUl1bvki81jvWs+vOEV75oud9q7RpImr33Uu+bKZdv9D3SB/tmlhf0nV5WkOe//2bvm/iz/B4iOd3d610yuedi75uoFDd41SNwp16VabVYkEjnnfX3zp+AAABcnAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJ749jAL7Wcdtk75ql1/x375o0+c+6/UlXt3eNJI393efeNYnNu31h7P8P/rOPS9Kdl270rkkPDPWu+eUX/+BdM7464l3Tn/+OLmZcAQEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADDBZKRI2MG7urxr5gz7V++aUwlMRvpvdyz0rpGkq1r/mlCdt0DAuyStIN+75t/9+3e9ayTpslT/iUUT8ca2G71rrv6XHX3QCSxwBQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEk5EioYkxJals3KfeNZemXOJd839Pn/CuuWJthneNJB2Zd613TWSc/3HcFR3eNY9cv9W75sHhTd41Z6R7V5x23d41I8Z+6V2DwYMrIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACaYjBQKpPlPPClJt4b8JyNNZMLKS1OC3jW/+a8rvWskKTs11bvmkoD/P6M0+R8nNZDI94uJ/d0m4uFD/+Bdkz17r/+BnPOvQb/EFRAAwAQBBAAw4R1A27Zt0+zZs1VQUKBAIKBNmzbFbV+wYIECgUDcMmvWrGT1CwAYJLwDqL29XcXFxVq1alWv+8yaNUuHDx+OLRs2bDivJgEAg4/33dOKigpVVFScc59gMKhwOJxwUwCAwa9P7gHV1tYqNzdX48aN06JFi3T06NFe9+3s7FQ0Go1bAACDX9IDaNasWXr55ZdVU1OjX/3qV6qrq1NFRYVOnz7d4/7V1dUKhUKxpbCwMNktAQD6oaT/HtDdd98d+3rixImaNGmSxo4dq9raWs2YMeOs/auqqrR06dLY62g0SggBwEWgzx/DHjNmjHJyctTU1NTj9mAwqKysrLgFADD49XkAHTx4UEePHlV+fn5fHwoAMIB4/wju+PHjcVczzc3N2rVrl7Kzs5Wdna1nnnlG8+bNUzgc1r59+/T444/rqquuUnl5eVIbBwAMbN4BtHPnTt16662x11/fv5k/f75Wr16t3bt367e//a2OHTumgoICzZw5Uz//+c8VDPrP5wUAGLy8A2j69Oly55gM8A9/+MN5NYQLz/XyhOK3eftosXdN2dB/9q65LGWId82otKHeNYnqdKf8iwL+Y556AWfOevLIRO+a5h8mMPGp6/CvwaDBXHAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABNJ/0huDEDdic2G/eVtvc+K3pu7r3/Yu+Zff3CJd03aCe8SSdLI33/pXfPZvGzvmoaFK71rFPAv2ZbgZNMflft/gGR325HEDoaLFldAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATDAZKRJ2+ljEuya19iPvmlF1CczC6fwnSpWk7oD/sToWT/GuGZqS4V1z5HS7d83CDY9510jSla31CdUBPrgCAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYILJSNH/JTixaCK++if/iUU/nb0qgSOle1f8LjLRu+bK/8Skoui/uAICAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABggslIMSilDB2aUN2d//kP3jXBgP/Eoie6T3rX/JdN5d41V4rJSNF/cQUEADBBAAEATHgFUHV1taZMmaLMzEzl5uZq7ty5amxsjNuno6NDlZWVGjFihC699FLNmzdPra2tSW0aADDweQVQXV2dKisrtX37dr377rvq6urSzJkz1d7eHtvn0Ucf1VtvvaU33nhDdXV1OnTokO64446kNw4AGNi8HkLYsmVL3Ot169YpNzdXDQ0NmjZtmiKRiF566SWtX79eP/zhDyVJa9eu1fe+9z1t375dN954Y/I6BwAMaOd1DygSiUiSsrOzJUkNDQ3q6upSWVlZbJ/x48dr1KhRqq/v+Wmczs5ORaPRuAUAMPglHEDd3d1asmSJbrrpJk2YMEGS1NLSooyMDA0fPjxu37y8PLW0tPT4PtXV1QqFQrGlsLAw0ZYAAANIwgFUWVmpPXv26NVXXz2vBqqqqhSJRGLLgQMHzuv9AAADQ0K/iLp48WK9/fbb2rZtm0aOHBlbHw6HdfLkSR07dizuKqi1tVXhcLjH9woGgwoGg4m0AQAYwLyugJxzWrx4sTZu3KitW7eqqKgobvvkyZOVnp6umpqa2LrGxkbt379fpaWlyekYADAoeF0BVVZWav369dq8ebMyMzNj93VCoZCGDBmiUCikBx54QEuXLlV2draysrL08MMPq7S0lCfgAABxvAJo9erVkqTp06fHrV+7dq0WLFggSXr++eeVkpKiefPmqbOzU+Xl5fr1r3+dlGYBAINHwDnnrJv4e9FoVKFQSNM1R2kJTPIISNJXc6cmVPfmi8971+SkDvOuWRfN9a7ZML7AuwawcMp1qVabFYlElJWV1et+zAUHADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADCR0CeiAhdS6jlm0+1N+wPHEjpWIjNbH+/u8K557r/9G++afH3oXQP0Z1wBAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMMFkpOj3AiH/yUj/41Uf9EEnPXvnRJ53Tf7zO/qgE2Bg4QoIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACSYjRf+X7n+a/uPQf0noUJ0u6F2z/H//k3fNqO4/e9cAgw1XQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwwGSn6vVP/5zPvmh9dW57YwZzzLhl98hP/w3hXAIMPV0AAABMEEADAhFcAVVdXa8qUKcrMzFRubq7mzp2rxsbGuH2mT5+uQCAQtzz00ENJbRoAMPB5BVBdXZ0qKyu1fft2vfvuu+rq6tLMmTPV3t4et9/ChQt1+PDh2LJixYqkNg0AGPi8HkLYsmVL3Ot169YpNzdXDQ0NmjZtWmz90KFDFQ6Hk9MhAGBQOq97QJFIRJKUnZ0dt/6VV15RTk6OJkyYoKqqKp04caLX9+js7FQ0Go1bAACDX8KPYXd3d2vJkiW66aabNGHChNj6e++9V6NHj1ZBQYF2796tJ554Qo2NjXrzzTd7fJ/q6mo988wzibYBABigAs4l8IsPkhYtWqTf//73+uCDDzRy5Mhe99u6datmzJihpqYmjR079qztnZ2d6uzsjL2ORqMqLCzUdM1RWiA9kdYApQwbllhhAv8c3MmT/jWnTnnXAAPFKdelWm1WJBJRVlZWr/sldAW0ePFivf3229q2bds5w0eSSkpKJKnXAAoGgwoGg4m0AQAYwLwCyDmnhx9+WBs3blRtba2Kioq+tWbXrl2SpPz8/IQaBAAMTl4BVFlZqfXr12vz5s3KzMxUS0uLJCkUCmnIkCHat2+f1q9fr9tuu00jRozQ7t279eijj2ratGmaNGlSn/wBAAADk1cArV69WtKZXzb9e2vXrtWCBQuUkZGh9957TytXrlR7e7sKCws1b948Pfnkk0lrGAAwOHj/CO5cCgsLVVdXd14NAQAuDsyGjUGp+xuzcwDof5iMFABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgIk06wa+yTknSTqlLskZNwMA8HZKXZL+9v95b/pdALW1tUmSPtA7xp0AAM5HW1ubQqFQr9sD7tsi6gLr7u7WoUOHlJmZqUAgELctGo2qsLBQBw4cUFZWllGH9hiHMxiHMxiHMxiHM/rDODjn1NbWpoKCAqWk9H6np99dAaWkpGjkyJHn3CcrK+uiPsG+xjicwTicwTicwTicYT0O57ry+RoPIQAATBBAAAATAyqAgsGgli9frmAwaN2KKcbhDMbhDMbhDMbhjIE0Dv3uIQQAwMVhQF0BAQAGDwIIAGCCAAIAmCCAAAAmBkwArVq1SldeeaUuueQSlZSU6E9/+pN1Sxfc008/rUAgELeMHz/euq0+t23bNs2ePVsFBQUKBALatGlT3HbnnJYtW6b8/HwNGTJEZWVl2rt3r02zfejbxmHBggVnnR+zZs2yabaPVFdXa8qUKcrMzFRubq7mzp2rxsbGuH06OjpUWVmpESNG6NJLL9W8efPU2tpq1HHf+C7jMH369LPOh4ceesio454NiAB67bXXtHTpUi1fvlwfffSRiouLVV5eriNHjli3dsFdd911Onz4cGz54IMPrFvqc+3t7SouLtaqVat63L5ixQq98MILWrNmjXbs2KFhw4apvLxcHR0dF7jTvvVt4yBJs2bNijs/NmzYcAE77Ht1dXWqrKzU9u3b9e6776qrq0szZ85Ue3t7bJ9HH31Ub731lt544w3V1dXp0KFDuuOOOwy7Tr7vMg6StHDhwrjzYcWKFUYd98INAFOnTnWVlZWx16dPn3YFBQWuurrasKsLb/ny5a64uNi6DVOS3MaNG2Ovu7u7XTgcds8++2xs3bFjx1wwGHQbNmww6PDC+OY4OOfc/Pnz3Zw5c0z6sXLkyBEnydXV1Tnnzvzdp6enuzfeeCO2zyeffOIkufr6eqs2+9w3x8E5537wgx+4Rx55xK6p76DfXwGdPHlSDQ0NKisri61LSUlRWVmZ6uvrDTuzsXfvXhUUFGjMmDG67777tH//fuuWTDU3N6ulpSXu/AiFQiopKbkoz4/a2lrl5uZq3LhxWrRokY4ePWrdUp+KRCKSpOzsbElSQ0ODurq64s6H8ePHa9SoUYP6fPjmOHztlVdeUU5OjiZMmKCqqiqdOHHCor1e9bvJSL/piy++0OnTp5WXlxe3Pi8vT59++qlRVzZKSkq0bt06jRs3TocPH9YzzzyjW265RXv27FFmZqZ1eyZaWlokqcfz4+ttF4tZs2bpjjvuUFFRkfbt26ef/vSnqqioUH19vVJTU63bS7ru7m4tWbJEN910kyZMmCDpzPmQkZGh4cOHx+07mM+HnsZBku69916NHj1aBQUF2r17t5544gk1NjbqzTffNOw2Xr8PIPxNRUVF7OtJkyappKREo0eP1uuvv64HHnjAsDP0B3fffXfs64kTJ2rSpEkaO3asamtrNWPGDMPO+kZlZaX27NlzUdwHPZfexuHBBx+MfT1x4kTl5+drxowZ2rdvn8aOHXuh2+xRv/8RXE5OjlJTU896iqW1tVXhcNioq/5h+PDhuuaaa9TU1GTdipmvzwHOj7ONGTNGOTk5g/L8WLx4sd5++229//77cR/fEg6HdfLkSR07dixu/8F6PvQ2Dj0pKSmRpH51PvT7AMrIyNDkyZNVU1MTW9fd3a2amhqVlpYadmbv+PHj2rdvn/Lz861bMVNUVKRwOBx3fkSjUe3YseOiPz8OHjyoo0ePDqrzwzmnxYsXa+PGjdq6dauKioritk+ePFnp6elx50NjY6P2798/qM6HbxuHnuzatUuS+tf5YP0UxHfx6quvumAw6NatW+f+8pe/uAcffNANHz7ctbS0WLd2Qf34xz92tbW1rrm52f3xj390ZWVlLicnxx05csS6tT7V1tbmPv74Y/fxxx87Se65555zH3/8sfvrX//qnHPul7/8pRs+fLjbvHmz2717t5szZ44rKipyX331lXHnyXWucWhra3OPPfaYq6+vd83Nze69995zN9xwg7v66qtdR0eHdetJs2jRIhcKhVxtba07fPhwbDlx4kRsn4ceesiNGjXKbd261e3cudOVlpa60tJSw66T79vGoampyf3sZz9zO3fudM3NzW7z5s1uzJgxbtq0acadxxsQAeSccy+++KIbNWqUy8jIcFOnTnXbt2+3bumCu+uuu1x+fr7LyMhwV1xxhbvrrrtcU1OTdVt97v3333eSzlrmz5/vnDvzKPZTTz3l8vLyXDAYdDNmzHCNjY22TfeBc43DiRMn3MyZM93ll1/u0tPT3ejRo93ChQsH3TdpPf35Jbm1a9fG9vnqq6/cj370I3fZZZe5oUOHuttvv90dPnzYruk+8G3jsH//fjdt2jSXnZ3tgsGgu+qqq9xPfvITF4lEbBv/Bj6OAQBgot/fAwIADE4EEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBM/D8kRtUnJp/mkwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b608e244",
      "metadata": {
        "id": "b608e244",
        "outputId": "440a4bfb-747e-4f89-b6f6-431da7fa3ef8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one\n"
          ]
        }
      ],
      "source": [
        "\n",
        "fake_images = []\n",
        "real_images = []\n",
        "with torch.no_grad():\n",
        "    generator.to('cpu')\n",
        "    discriminator.to('cpu')\n",
        "\n",
        "    for i, (imgs,label) in enumerate(train_loader):\n",
        "        real_images = imgs\n",
        "        \n",
        "        #caption = random.randint(0, 9)   \n",
        "        #caption = torch.tensor(caption, dtype=torch.int64)\n",
        "        #latent = torch.rand_like(torch.Tensor(1,100))\n",
        "        #fake_images = generator(latent,caption)\n",
        "        \n",
        "        \n",
        "        for i in range(BATCH_SIZE):\n",
        "          caption = label[i]\n",
        "          latent = torch.rand_like(torch.Tensor(1,100))\n",
        "          fake_image = generator(latent,caption)\n",
        "\n",
        "          if (i==0):\n",
        "            print('one')\n",
        "            fake_images = generator(latent,caption)\n",
        "          else:\n",
        "            fake_images = torch.cat((fake_images, fake_image), 0)\n",
        "        break\n",
        "    \n",
        "    #fake_images = fake_images[:BATCH_SIZE,:,:,:]\n",
        "    #real_images = real_images[:BATCH_SIZE,:,:,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48014174",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48014174",
        "outputId": "ef3b9ed4-b7ac-42fa-d516-bf520423d4e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inception score: 0.43\n"
          ]
        }
      ],
      "source": [
        "#%% Inception score\n",
        "from torchvision.models import inception_v3\n",
        "from scipy.stats import entropy\n",
        "\n",
        "with torch.no_grad():\n",
        "    images = fake_images\n",
        "    batch_size = BATCH_SIZE\n",
        "    resize=True\n",
        "    \n",
        "    # Load pre-trained Inception-v3 model\n",
        "    model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
        "    model.eval()\n",
        "    model.requires_grad_ = False\n",
        "\n",
        "    \n",
        "    # Prepare the images\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "    if resize:\n",
        "        images = torch.cat([images,images,images],dim=1)\n",
        "        images = transforms.Compose([\n",
        "            transforms.Resize((299, 299)),\n",
        "            transforms.Grayscale(num_output_channels=3),\n",
        "        ])(images)\n",
        "    else:\n",
        "        images = [torch.from_numpy(image.transpose(2, 0, 1)).float().div(255).unsqueeze(0) for image in images]\n",
        "    \n",
        "    # Compute the predictions\n",
        "    n_images = images.shape[0]\n",
        "    n_batches = int(np.ceil(n_images / batch_size))\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(n_batches):\n",
        "            start_idx = i * batch_size\n",
        "            end_idx = min((i + 1) * batch_size, n_images)\n",
        "            batch = images.to(device)\n",
        "            #batch = torch.cat(images[start_idx:end_idx], dim=0).to(device)\n",
        "            pred = model(batch.detach())\n",
        "            pred = F.softmax(pred, dim=1).cpu().numpy()\n",
        "            preds.append(pred)\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "    \n",
        "    # Compute the Inception Score\n",
        "    scores = []\n",
        "    for i in range(preds.shape[0]):\n",
        "        p_yx = preds[i]\n",
        "        p_y = np.expand_dims(np.mean(p_yx, axis=0), axis=0)\n",
        "        scores.append(entropy(p_yx.T, p_y.T))\n",
        "    kl_divergence = np.mean(scores)\n",
        "    entropy_y = entropy(np.mean(preds, axis=0))\n",
        "    inception_score = np.exp(kl_divergence - entropy_y)\n",
        "    print(f'Inception score: {inception_score:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FID score\n",
        "\n",
        "f_images = torch.cat([fake_images, fake_images, fake_images], dim=1)\n",
        "f_images = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "])(f_images)\n",
        "\n",
        "\n",
        "# load a pre-trained Inception-v3 model\n",
        "inception_model = inception_v3(pretrained=True, aux_logits=True,)\n",
        "inception_model.to(device)\n",
        "inception_model.eval()\n",
        "\n",
        "# compute the feature representations of the real and fake images\n",
        "real_features = []\n",
        "fake_features = []\n",
        "for batch in train_loader:\n",
        "    images, _ = batch\n",
        "    images = images.to(device)\n",
        "    with torch.no_grad():\n",
        "        images = torch.cat([images,images,images],dim=1)\n",
        "        images = transforms.Compose([\n",
        "            transforms.Resize((299, 299)),\n",
        "            transforms.Grayscale(num_output_channels=3),\n",
        "        ])(images)\n",
        "\n",
        "           \n",
        "        features = inception_model(images).view(images.size(0), -1)\n",
        "    real_features.append(features.cpu().numpy())\n",
        "    break\n",
        "with torch.no_grad():\n",
        "    features = inception_model(f_images).view(f_images.size(0), -1)\n",
        "fake_features.append(features.cpu().numpy())\n",
        "\n",
        "# calculate the mean and covariance of the feature representations\n",
        "real_features = np.concatenate(real_features, axis=0)\n",
        "fake_features = np.concatenate(fake_features, axis=0)\n",
        "mu1, sigma1 = np.mean(real_features, axis=0), np.cov(real_features, rowvar=False)\n",
        "mu2, sigma2 = np.mean(fake_features, axis=0), np.cov(fake_features, rowvar=False)\n",
        "\n",
        "# calculate the FID score\n",
        "mu_diff = mu1 - mu2\n",
        "sigma_diff_sqrt = linalg.sqrtm(sigma1 @ sigma2)\n",
        "fid_score = np.real(np.trace(sigma1 + sigma2 - 2*sigma_diff_sqrt)) + np.dot(mu_diff, mu_diff)\n",
        "print(f'FID score: {fid_score:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUpzfww27ah2",
        "outputId": "79f91c38-48d4-4466-a70b-595a387e6829"
      },
      "id": "gUpzfww27ah2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FID score: 338.79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "645ba33b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "645ba33b",
        "outputId": "a2db329b-fa28-4df5-c0f0-7559df22ecae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.9/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "LPIPS?  0.1901334673166275\n"
          ]
        }
      ],
      "source": [
        "#LPIPS metric \n",
        "loss_fn_alex = lpips.LPIPS(net='alex') \n",
        "\n",
        "lpip_fake = colorize(fake_images)\n",
        "lpip_real = colorize(real_images) # !! images must be rbg and within [-1,1]\n",
        "\n",
        "lpip_fake = transforms.Compose([\n",
        "    transforms.Resize((64, 64))])(lpip_fake)\n",
        "\n",
        "lpip_real = transforms.Compose([\n",
        "    transforms.Resize((64, 64))])(lpip_real)\n",
        "\n",
        "\n",
        "\n",
        "d = loss_fn_alex(lpip_fake, lpip_real)\n",
        "print(\"LPIPS? \",d[0].item())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# create a tensor\n",
        "my_tensor = d\n",
        "\n",
        "# get minimum value\n",
        "min_val = torch.min(my_tensor)\n",
        "\n",
        "# get maximum value\n",
        "max_val = torch.max(my_tensor)\n",
        "\n",
        "# get mean value\n",
        "mean_val = torch.mean(my_tensor)\n",
        "\n",
        "# print the results\n",
        "print(\"Minimum value:\", min_val.item())\n",
        "print(\"Maximum value:\", max_val.item())\n",
        "print(\"Mean value:\", mean_val.item())\n"
      ],
      "metadata": {
        "id": "cl7UvjhH4btt",
        "outputId": "4dc5ca6e-5695-4936-afbf-c3fa6d9a904e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cl7UvjhH4btt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum value: 0.07334506511688232\n",
            "Maximum value: 0.30068492889404297\n",
            "Mean value: 0.16741244494915009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# create two tensors\n",
        "y_true = real_images\n",
        "y_pred = fake_images\n",
        "\n",
        "# create a criterion for calculating MSE\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# calculate MSE\n",
        "mse = criterion(y_pred, y_true)\n",
        "\n",
        "# print the result\n",
        "print(\"MSE:\", mse.item())\n"
      ],
      "metadata": {
        "id": "mCPTGTiuIHM_",
        "outputId": "7cbf43c2-3f67-4853-ee13-1b5515ef4c22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mCPTGTiuIHM_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.3464761972427368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex1 = real_images[:32]\n",
        "ex2 = fake_images[:32]\n",
        "\n",
        "f, axarr = plt.subplots(2)\n",
        "axarr[0].imshow(ex1[0][0])\n",
        "axarr[1].imshow(ex2[0][0])   \n"
      ],
      "metadata": {
        "id": "G6ZqslHzIe5d",
        "outputId": "435ceeb4-d9e0-4690-f909-59347ef76512",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        }
      },
      "id": "G6ZqslHzIe5d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0d72912a90>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate mean of images\n",
        "mist_means = []\n",
        "number_mean = []\n",
        "max = 1000\n",
        "\n",
        "#for each number(0,9)\n",
        "for num in range(1):\n",
        "\n",
        "  #for each image in dataset\n",
        "  for k,(images,labels) in enumerate(test_loader):  \n",
        "    for i in range(len(labels[:])):\n",
        "    \n",
        "      #if it's the number we want\n",
        "      if num == labels[i].item():\n",
        "        number_mean.append(images[i])\n",
        "        \n",
        "        if len(number_mean) >= max:\n",
        "          break\n",
        "  number_mean = torch.stack(number_mean,dim=0)\n",
        "  \n"
      ],
      "metadata": {
        "id": "luey8v0fkvxC"
      },
      "id": "luey8v0fkvxC",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_mean.shape"
      ],
      "metadata": {
        "id": "DhDQeO0QnrgN",
        "outputId": "0811641d-7f88-4ef0-8c14-c1d136b3fb08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DhDQeO0QnrgN",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000, 1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "number_mean = torch.mean(number_mean, dim=0)"
      ],
      "metadata": {
        "id": "dA030ennqMDV"
      },
      "id": "dA030ennqMDV",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_mean.shape"
      ],
      "metadata": {
        "id": "61XEsABvqW0v",
        "outputId": "395efe71-1653-4f96-fc9b-24e5b68b1fcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "61XEsABvqW0v",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " f, axarr = plt.subplots(1)\n",
        " axarr.imshow(number_mean[0])"
      ],
      "metadata": {
        "id": "2dzvEkg4qTWO",
        "outputId": "c8f1e7c3-5986-4d61-def6-8ba3d88a5899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        }
      },
      "id": "2dzvEkg4qTWO",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb6a75b2610>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}