{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install lpips"
      ],
      "metadata": {
        "id": "POQbrwGa19Ji",
        "outputId": "ce212e27-9862-47a1-c272-33298b8a3f3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "POQbrwGa19Ji",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting lpips\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 KB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.9/dist-packages (from lpips) (4.65.0)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from lpips) (0.15.1+cu118)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from lpips) (1.10.1)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from lpips) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.9/dist-packages (from lpips) (1.22.4)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips) (3.10.7)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips) (3.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=0.4.0->lpips) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=0.4.0->lpips) (3.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.2.1->lpips) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.2.1->lpips) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=0.4.0->lpips) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.2.1->lpips) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.2.1->lpips) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.2.1->lpips) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.2.1->lpips) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=0.4.0->lpips) (1.3.0)\n",
            "Installing collected packages: lpips\n",
            "Successfully installed lpips-0.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a493c415",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a493c415",
        "outputId": "050ec35e-be5e-4b8d-b699-8286427c1510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:cpu\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Created on Tue Jan 17 13:24:23 2023\n",
        "\n",
        "@author: DIMITRIS\n",
        "\n",
        "Description: Conditional GAN Network, for eaducational purposed\n",
        "Status: Working, needs some fine-tuning\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# %% Import and stuff\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import torchvision.utils as vutils\n",
        "from  torch.utils import data\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "from scipy import linalg\n",
        "import lpips\n",
        "\n",
        "\n",
        "NUM_EPOCHS = 150\n",
        "LR = 0.0002\n",
        "LATENT_DIM = 100\n",
        "IMG_SIZE = 28\n",
        "CHANNELS = 1\n",
        "B1 = 0.5\n",
        "B2 = 0.999\n",
        "\n",
        "\n",
        "GEN_STATE_DICT = \"gen_state_dict\"\n",
        "DISC_STATE_DICT = \"disc_state_dict\"\n",
        "GEN_OPTIMIZER = \"gen_optimizer\"\n",
        "DISC_OPTIMIZER = \"disc_optimizer\"\n",
        "G_LOSSES = \"g_losses\"\n",
        "D_LOSSES = \"d_losses\"\n",
        "\n",
        "\n",
        "\n",
        "SHUFFLE = True\n",
        "PIN_MEMORY = True\n",
        "NUM_WORKERS = 0\n",
        "#small batch size cause inception_v3 hogs ram\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "specific_latent = torch.tensor([[0.7628, 0.1779, 0.3978, 0.3606, 0.6387,\n",
        "         0.3044, 0.8340, 0.3884, 0.9313, 0.5635, 0.1994, 0.6934, 0.5326,\n",
        "         0.3676, 0.5342, 0.9480, 0.4120, 0.5845, 0.4035, 0.5298, 0.0177,\n",
        "         0.5605, 0.6453, 0.9576, 0.7153, 0.1923, 0.8122, 0.0937, 0.5744,\n",
        "         0.5951, 0.8890, 0.4838, 0.5707, 0.6760, 0.3738, 0.2796, 0.1549,\n",
        "         0.8220, 0.2800, 0.4051, 0.2553, 0.1831, 0.0046, 0.9021, 0.0264,\n",
        "         0.2327, 0.8261, 0.0534, 0.1582, 0.4087, 0.9047, 0.1409, 0.6864,\n",
        "         0.1439, 0.3432, 0.1072, 0.5907, 0.6756, 0.6942, 0.6814, 0.3368,\n",
        "         0.4138, 0.8030, 0.7024, 0.3309, 0.7288, 0.2193, 0.1954, 0.9948,\n",
        "         0.1201, 0.9483, 0.7407, 0.4849, 0.6500, 0.8649, 0.7405, 0.4725,\n",
        "         0.5373, 0.6541, 0.5444, 0.7425, 0.8940, 0.3580, 0.3905, 0.8924,\n",
        "         0.2995, 0.3726, 0.5399, 0.3057, 0.3380, 0.8313, 0.1137, 0.0120,\n",
        "         0.7714, 0.2561, 0.2569, 0.2994, 0.7648, 0.2413, 0.6101\n",
        "        ]])\n",
        "\n",
        "\n",
        "img_shape = (CHANNELS, IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Device:{}'.format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "56f4e4cf",
      "metadata": {
        "id": "56f4e4cf"
      },
      "outputs": [],
      "source": [
        "# %% helper funcitons\n",
        "\n",
        "\n",
        "def save_checkpoint(state, filename):\n",
        "    print(\"=> Saving chekpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint):\n",
        "    generator.load_state_dict(checkpoint[GEN_STATE_DICT])\n",
        "    optimizer_G.load_state_dict(checkpoint[GEN_OPTIMIZER])\n",
        "    discriminator.load_state_dict(checkpoint[DISC_STATE_DICT])\n",
        "    optimizer_D.load_state_dict(checkpoint[DISC_OPTIMIZER])\n",
        "    G_losses = checkpoint[G_LOSSES]\n",
        "    D_losses = checkpoint[D_LOSSES]\n",
        "    \n",
        "\n",
        "\n",
        "# takes input tensor and return a tensor of same size but every element has different value\n",
        "def build_fake_labels(old_list):\n",
        "  \n",
        "    new_list = []\n",
        "\n",
        "    for i, x in enumerate(old_list):\n",
        "\n",
        "        if (i % 10) != x:\n",
        "            new_list.append(i % 10)\n",
        "        else:\n",
        "            new_list.append((x.item()+1) % 10)\n",
        "\n",
        "    return torch.tensor(new_list, dtype=torch.int64).to(device)\n",
        "\n",
        "\n",
        "def add_noise(inputs, variance):\n",
        "    noise = torch.randn_like(inputs)\n",
        "    return inputs + variance*noise\n",
        "\n",
        "def gen_image(caption=-1,randomLatent=True):\n",
        "    generator.to('cpu')\n",
        "    discriminator.to('cpu')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for image,_ in train_loader:\n",
        "            f, axarr = plt.subplots(1)\n",
        "            \n",
        "            if randomLatent:\n",
        "                latent = torch.rand_like(torch.Tensor(1,100))\n",
        "            else:\n",
        "                latent = specific_latent\n",
        "                \n",
        "            if caption == -1:\n",
        "                caption = random.randint(0, 9)\n",
        "            \n",
        "            caption = torch.tensor(caption, dtype=torch.int64)\n",
        "            fake_image = generator(latent,caption)  \n",
        "           \n",
        "            \n",
        "            #axarr.imshow(add_noise(image[0][0],0.5))    \n",
        "            axarr.imshow(fake_image[0][0])   \n",
        "            print(\"Supposed to be %d\" %caption.item())\n",
        "    \n",
        "            break\n",
        "        \n",
        "def discriminate_image(caption=-1,genOrReal=0):#random.randint(0, 1)):\n",
        "    generator.to('cpu')\n",
        "    discriminator.to('cpu')\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for  i, (imgs, labels) in enumerate(example_loader):\n",
        "            f, axarr = plt.subplots(1)\n",
        "            \n",
        "            fake_labels = build_fake_labels(labels.to(device))\n",
        "            labels = labels.to('cpu')\n",
        "            z = Variable(Tensor(np.random.normal(0, 1, (1,LATENT_DIM)))).cpu()\n",
        "            if caption == -1:\n",
        "                caption = random.randint(0, 9)\n",
        "            caption = torch.tensor(caption, dtype=torch.int64)\n",
        "            \n",
        "            \n",
        "            #feed discriminator fake image, expect \"0\" output\n",
        "            if genOrReal == 0:\n",
        "                fake_image = generator(z,caption)\n",
        "                axarr.imshow(fake_image[0].reshape(-1, 28, 28)[0])\n",
        "                pred = discriminator(fake_image,caption).detach()\n",
        "                print(\"Discriminator Prediction: {},Should be: {}, label = {}\".format(pred,\"0\",caption))\n",
        "            #feed discriminator real image, expect \"1\" output\n",
        "            else:\n",
        "                fake_image = generator(z,labels[0])\n",
        "                axarr.imshow(imgs[0].reshape(-1, 28, 28)[0])\n",
        "                pred = discriminator(imgs.detach(),labels[0].detach()).detach()\n",
        "                print(\"Discriminator Prediction: {},Should be: {}, label= {}\".format(pred,\"1\",labels[0]+1))\n",
        "            \n",
        "    \n",
        "            break        \n",
        "\n",
        "# Check if a tensor image is normalized in [-1,1]\n",
        "# returns true if it s within [-1,1]\n",
        "def is_normalized(image):\n",
        "  \n",
        "  tensor_image_normalized = image[0]  # your tensor image here\n",
        "\n",
        "  min_value = tensor_image_normalized.min().item()\n",
        "  max_value = tensor_image_normalized.max().item()\n",
        "\n",
        "  \n",
        "  return (min_value >= -1.0 and max_value <= 1.0)\n",
        "\n",
        "\n",
        "def colorize(image):\n",
        "  return torch.cat([image, image, image], dim=1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e8e59f16",
      "metadata": {
        "id": "e8e59f16",
        "outputId": "95a4d1dc-cd3a-4230-eac8-9aa7adec38d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /root/torch_datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 63116100.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/torch_datasets/MNIST/raw/train-images-idx3-ubyte.gz to /root/torch_datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /root/torch_datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 69578227.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/torch_datasets/MNIST/raw/train-labels-idx1-ubyte.gz to /root/torch_datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /root/torch_datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 22610040.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/torch_datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to /root/torch_datasets/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /root/torch_datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 12402688.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/torch_datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to /root/torch_datasets/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# %%train data\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "    ])\n",
        "\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"~/torch_datasets\", train=True, transform=transform, download=True\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"~/torch_datasets\", train=False, transform=transform, download=True\n",
        ")\n",
        "\n",
        "train_loader = data.DataLoader(\n",
        "                                train_dataset,\n",
        "                                batch_size=BATCH_SIZE,\n",
        "                                shuffle=False,\n",
        "                                num_workers=NUM_WORKERS,\n",
        "                                pin_memory=False\n",
        "                                )\n",
        "\n",
        "test_loader = data.DataLoader(\n",
        "                                test_dataset,\n",
        "                                batch_size=32,\n",
        "                                shuffle=True,\n",
        "                                num_workers=0\n",
        "                                )\n",
        "\n",
        "example_loader = data.DataLoader(\n",
        "                                train_dataset,\n",
        "                                batch_size=1,\n",
        "                                shuffle=True,\n",
        "                                num_workers=0,\n",
        "                                drop_last=True,\n",
        "                                )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "97624d9f",
      "metadata": {
        "id": "97624d9f"
      },
      "outputs": [],
      "source": [
        "# %% Detective: fake or no fake -> 1 output [0, 1]\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(2, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 1)\n",
        "\n",
        "        self.emb = nn.Embedding(10, 50)\n",
        "        self.emb_fc = nn.Linear(50, 784)\n",
        "\n",
        "        self.nconv1 = nn.Conv2d(2, 64, kernel_size=5)\n",
        "        self.nconv2 = nn.Conv2d(64, 128, kernel_size=5)\n",
        "        self.pool = nn.AvgPool2d(kernel_size=3)\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=2)\n",
        "        self.nfc1 = nn.Linear(1152, 164)\n",
        "        self.nfc2 = nn.Linear(164, 1)\n",
        "\n",
        "    # oldWay flag to select between 2 train methods, not sure which is best yet\n",
        "    def forward(self, x, c, oldWay=False):\n",
        "\n",
        "        c = self.emb(c)\n",
        "        c = self.emb_fc(c)\n",
        "        c = c.view(-1, 1, 28, 28)\n",
        "        x = torch.cat((c, x), 1)  # concat image[1,28,28] with text [1,28,28]\n",
        "\n",
        "        x = F.leaky_relu(self.nconv1(x))\n",
        "        x = F.leaky_relu(self.nconv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(-1, 1152)\n",
        "        x = F.leaky_relu(self.nfc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.nfc2(x)\n",
        "\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# %% Generate Fake Data: output like real data [1, 28, 28] and values -1, 1\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(LATENT_DIM, 7*7*63)  # [n,100]->[n,3087]\n",
        "        self.ct1 = nn.ConvTranspose2d(64, 32, 4, stride=2)  # [n, 64, 16, 16] [32,..,..]\n",
        "        self.ct2 = nn.ConvTranspose2d(32, 16, 4, stride=2)  # [n, 32, , ]->[n, 16, 34, 34]\n",
        "        self.conv = nn.Conv2d(16, 1, kernel_size=7)  # [n, 16, 34, 34]-> [n, 1, 28, 28]\n",
        "        \n",
        "        self.emb = nn.Embedding(10, 50) \n",
        "        self.label_lin = nn.Linear(50, 49)\n",
        "        self.conv_x_c = nn.ConvTranspose2d(65, 64, 4, stride=2)  # upsample [65,7,7] -> [64,14,14]\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        # Pass latent space input into linear layer and reshape\n",
        "        x = self.lin1(x)  # (n,100) -> (n,3187)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = x.view(-1, 63, 7, 7)  # (n,3187) -> (63,7,7)\n",
        "        \n",
        "        #Encode label\n",
        "        c = self.emb(c)  # (n,) -> (n,50)\n",
        "        c = self.label_lin(c)  # (n,50) -> (n,49)\n",
        "        c = c.view(-1, 1, 7, 7)  # (n,49) -> (n,1,7,7)\n",
        "        x = torch.cat((c, x), 1) # concat image[63,7,7] with text [1,7,7]\n",
        "\n",
        "        x = self.ct1(x)  # [n, 64, 16, 16] [32,34,34]\n",
        "        x = F.leaky_relu(x)\n",
        "\n",
        "        # Upsample to 34x34 (16 feature maps)\n",
        "        x = self.ct2(x)\n",
        "        x = F.leaky_relu(x)\n",
        "\n",
        "        # Convolution to 28x28 (1 feature map)\n",
        "        x = self.tanh(self.conv(x))\n",
        "        return x\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "abc07c4f",
      "metadata": {
        "id": "abc07c4f"
      },
      "outputs": [],
      "source": [
        "#%% Loss fucntion, optimizers\n",
        "loss_func = nn.BCELoss()\n",
        "d_loss_func = nn.BCELoss()\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    loss_func.cuda()\n",
        "\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=LR,betas=(B1 ,B2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=LR,betas=(B1 ,B2))\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "9e61a176",
      "metadata": {
        "id": "9e61a176"
      },
      "outputs": [],
      "source": [
        "#if you're running this on colab, download corresponding chackpoint file and upload it to runtime\n",
        "load_checkpoint(torch.load(\"cond_gan_pytorch10.pth.tar\",map_location=(device)))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test image generation\n",
        "gen_image()"
      ],
      "metadata": {
        "id": "fmyJkwav21hA",
        "outputId": "bd70ab88-f4ce-42d9-b8e1-840f1b4c6601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        }
      },
      "id": "fmyJkwav21hA",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supposed to be 4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "b608e244",
      "metadata": {
        "id": "b608e244",
        "outputId": "440a4bfb-747e-4f89-b6f6-431da7fa3ef8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one\n"
          ]
        }
      ],
      "source": [
        "\n",
        "fake_images = []\n",
        "real_images = []\n",
        "with torch.no_grad():\n",
        "    generator.to('cpu')\n",
        "    discriminator.to('cpu')\n",
        "\n",
        "    for i, (imgs,label) in enumerate(train_loader):\n",
        "        real_images = imgs\n",
        "        \n",
        "        #caption = random.randint(0, 9)   \n",
        "        #caption = torch.tensor(caption, dtype=torch.int64)\n",
        "        #latent = torch.rand_like(torch.Tensor(1,100))\n",
        "        #fake_images = generator(latent,caption)\n",
        "        \n",
        "        \n",
        "        for i in range(BATCH_SIZE):\n",
        "          caption = label[i]\n",
        "          latent = torch.rand_like(torch.Tensor(1,100))\n",
        "          fake_image = generator(latent,caption)\n",
        "\n",
        "          if (i==0):\n",
        "            print('one')\n",
        "            fake_images = generator(latent,caption)\n",
        "          else:\n",
        "            fake_images = torch.cat((fake_images, fake_image), 0)\n",
        "        break\n",
        "    \n",
        "    #fake_images = fake_images[:BATCH_SIZE,:,:,:]\n",
        "    #real_images = real_images[:BATCH_SIZE,:,:,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "48014174",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48014174",
        "outputId": "ef3b9ed4-b7ac-42fa-d516-bf520423d4e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inception score: 0.43\n"
          ]
        }
      ],
      "source": [
        "#%% Inception score\n",
        "from torchvision.models import inception_v3\n",
        "from scipy.stats import entropy\n",
        "\n",
        "with torch.no_grad():\n",
        "    images = fake_images\n",
        "    batch_size = BATCH_SIZE\n",
        "    resize=True\n",
        "    \n",
        "    # Load pre-trained Inception-v3 model\n",
        "    model = inception_v3(pretrained=True, transform_input=False).to(device)\n",
        "    model.eval()\n",
        "    model.requires_grad_ = False\n",
        "\n",
        "    \n",
        "    # Prepare the images\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "    if resize:\n",
        "        images = torch.cat([images,images,images],dim=1)\n",
        "        images = transforms.Compose([\n",
        "            transforms.Resize((299, 299)),\n",
        "            transforms.Grayscale(num_output_channels=3),\n",
        "        ])(images)\n",
        "    else:\n",
        "        images = [torch.from_numpy(image.transpose(2, 0, 1)).float().div(255).unsqueeze(0) for image in images]\n",
        "    \n",
        "    # Compute the predictions\n",
        "    n_images = images.shape[0]\n",
        "    n_batches = int(np.ceil(n_images / batch_size))\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(n_batches):\n",
        "            start_idx = i * batch_size\n",
        "            end_idx = min((i + 1) * batch_size, n_images)\n",
        "            batch = images.to(device)\n",
        "            #batch = torch.cat(images[start_idx:end_idx], dim=0).to(device)\n",
        "            pred = model(batch.detach())\n",
        "            pred = F.softmax(pred, dim=1).cpu().numpy()\n",
        "            preds.append(pred)\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "    \n",
        "    # Compute the Inception Score\n",
        "    scores = []\n",
        "    for i in range(preds.shape[0]):\n",
        "        p_yx = preds[i]\n",
        "        p_y = np.expand_dims(np.mean(p_yx, axis=0), axis=0)\n",
        "        scores.append(entropy(p_yx.T, p_y.T))\n",
        "    kl_divergence = np.mean(scores)\n",
        "    entropy_y = entropy(np.mean(preds, axis=0))\n",
        "    inception_score = np.exp(kl_divergence - entropy_y)\n",
        "    print(f'Inception score: {inception_score:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FID score\n",
        "\n",
        "f_images = torch.cat([fake_images, fake_images, fake_images], dim=1)\n",
        "f_images = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "])(f_images)\n",
        "\n",
        "\n",
        "# load a pre-trained Inception-v3 model\n",
        "inception_model = inception_v3(pretrained=True, aux_logits=True,)\n",
        "inception_model.to(device)\n",
        "inception_model.eval()\n",
        "\n",
        "# compute the feature representations of the real and fake images\n",
        "real_features = []\n",
        "fake_features = []\n",
        "for batch in train_loader:\n",
        "    images, _ = batch\n",
        "    images = images.to(device)\n",
        "    with torch.no_grad():\n",
        "        images = torch.cat([images,images,images],dim=1)\n",
        "        images = transforms.Compose([\n",
        "            transforms.Resize((299, 299)),\n",
        "            transforms.Grayscale(num_output_channels=3),\n",
        "        ])(images)\n",
        "\n",
        "           \n",
        "        features = inception_model(images).view(images.size(0), -1)\n",
        "    real_features.append(features.cpu().numpy())\n",
        "    break\n",
        "with torch.no_grad():\n",
        "    features = inception_model(f_images).view(f_images.size(0), -1)\n",
        "fake_features.append(features.cpu().numpy())\n",
        "\n",
        "# calculate the mean and covariance of the feature representations\n",
        "real_features = np.concatenate(real_features, axis=0)\n",
        "fake_features = np.concatenate(fake_features, axis=0)\n",
        "mu1, sigma1 = np.mean(real_features, axis=0), np.cov(real_features, rowvar=False)\n",
        "mu2, sigma2 = np.mean(fake_features, axis=0), np.cov(fake_features, rowvar=False)\n",
        "\n",
        "# calculate the FID score\n",
        "mu_diff = mu1 - mu2\n",
        "sigma_diff_sqrt = linalg.sqrtm(sigma1 @ sigma2)\n",
        "fid_score = np.real(np.trace(sigma1 + sigma2 - 2*sigma_diff_sqrt)) + np.dot(mu_diff, mu_diff)\n",
        "print(f'FID score: {fid_score:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUpzfww27ah2",
        "outputId": "79f91c38-48d4-4466-a70b-595a387e6829"
      },
      "id": "gUpzfww27ah2",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FID score: 338.79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "645ba33b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "645ba33b",
        "outputId": "a2db329b-fa28-4df5-c0f0-7559df22ecae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.9/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "LPIPS?  0.1901334673166275\n"
          ]
        }
      ],
      "source": [
        "#LPIPS metric \n",
        "loss_fn_alex = lpips.LPIPS(net='alex') \n",
        "\n",
        "lpip_fake = colorize(fake_images)\n",
        "lpip_real = colorize(real_images) # !! images must be rbg and within [-1,1]\n",
        "\n",
        "lpip_fake = transforms.Compose([\n",
        "    transforms.Resize((64, 64))])(lpip_fake)\n",
        "\n",
        "lpip_real = transforms.Compose([\n",
        "    transforms.Resize((64, 64))])(lpip_real)\n",
        "\n",
        "\n",
        "\n",
        "d = loss_fn_alex(lpip_fake, lpip_real)\n",
        "print(\"LPIPS? \",d[0].item())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# create a tensor\n",
        "my_tensor = d\n",
        "\n",
        "# get minimum value\n",
        "min_val = torch.min(my_tensor)\n",
        "\n",
        "# get maximum value\n",
        "max_val = torch.max(my_tensor)\n",
        "\n",
        "# get mean value\n",
        "mean_val = torch.mean(my_tensor)\n",
        "\n",
        "# print the results\n",
        "print(\"Minimum value:\", min_val.item())\n",
        "print(\"Maximum value:\", max_val.item())\n",
        "print(\"Mean value:\", mean_val.item())\n"
      ],
      "metadata": {
        "id": "cl7UvjhH4btt",
        "outputId": "4dc5ca6e-5695-4936-afbf-c3fa6d9a904e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cl7UvjhH4btt",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum value: 0.07334506511688232\n",
            "Maximum value: 0.30068492889404297\n",
            "Mean value: 0.16741244494915009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# create two tensors\n",
        "y_true = real_images\n",
        "y_pred = fake_images\n",
        "\n",
        "# create a criterion for calculating MSE\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# calculate MSE\n",
        "mse = criterion(y_pred, y_true)\n",
        "\n",
        "# print the result\n",
        "print(\"MSE:\", mse.item())\n"
      ],
      "metadata": {
        "id": "mCPTGTiuIHM_",
        "outputId": "7cbf43c2-3f67-4853-ee13-1b5515ef4c22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mCPTGTiuIHM_",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.3464761972427368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex1 = real_images[:32]\n",
        "ex2 = fake_images[:32]\n",
        "\n",
        "f, axarr = plt.subplots(2)\n",
        "axarr[0].imshow(ex1[0][0])\n",
        "axarr[1].imshow(ex2[0][0])   \n"
      ],
      "metadata": {
        "id": "G6ZqslHzIe5d",
        "outputId": "435ceeb4-d9e0-4690-f909-59347ef76512",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        }
      },
      "id": "G6ZqslHzIe5d",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0d72912a90>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}