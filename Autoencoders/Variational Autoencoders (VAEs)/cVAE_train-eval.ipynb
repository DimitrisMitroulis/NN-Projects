{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install lpips"
      ],
      "metadata": {
        "id": "WIdHIawmddJG",
        "outputId": "5e795715-8559-4ce3-f10e-77664ec84344",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "WIdHIawmddJG",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lpips in /usr/local/lib/python3.9/dist-packages (0.1.4)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.9/dist-packages (from lpips) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from lpips) (1.10.1)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from lpips) (0.15.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.9/dist-packages (from lpips) (4.65.0)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from lpips) (2.0.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips) (3.11.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips) (3.1.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=0.4.0->lpips) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=0.4.0->lpips) (3.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.2.1->lpips) (8.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.2.1->lpips) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=0.4.0->lpips) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.2.1->lpips) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.2.1->lpips) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.2.1->lpips) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.2.1->lpips) (2.0.12)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=0.4.0->lpips) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3bda6f8f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bda6f8f",
        "outputId": "349bfa3b-f5ac-4a0e-bc1b-152f06b0dd6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:cpu\n"
          ]
        }
      ],
      "source": [
        "# %% Import and stuff\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import torchvision.utils as vutils\n",
        "from  torch.utils import data\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "import matplotlib.colors as mcolors\n",
        "import os\n",
        "import gc\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "from torch.utils.data import Subset\n",
        "from datetime import datetime \n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as tran\n",
        "from scipy import linalg\n",
        "import lpips\n",
        "\n",
        "from scipy.stats import entropy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "NUM_EPOCHS = 60\n",
        "LR = 0.0008\n",
        "LATENT_SPACE_SIZE = 20\n",
        "IMG_SIZE = 28\n",
        "CHANNELS = 1\n",
        "B1 = 0.5\n",
        "B2 = 0.999\n",
        "RANDOM_SEED = 123\n",
        "\n",
        "\n",
        "STATE_DICT = \"state_dict\"\n",
        "MODEL_OPTIMIZER = \"model_optimizer\"\n",
        "LOSSES = \"losses\"\n",
        "RECON_LOSS = \"recon_loss\"\n",
        "KL_DIV = \"kl_div\"\n",
        "\n",
        "SHUFFLE = True\n",
        "PIN_MEMORY = True\n",
        "NUM_WORKERS = 0\n",
        "BATCH_SIZE = 2000\n",
        "\n",
        "specific_latent = torch.tensor([[0.7628, 0.1779, 0.3978, 0.3606, 0.6387,\n",
        "         0.3044, 0.8340, 0.3884, 0.9313, 0.5635, 0.1994, 0.6934, 0.5326,\n",
        "         0.3676, 0.5342, 0.9480, 0.4120, 0.5845, 0.4035, 0.5298, 0.0177,\n",
        "         0.5605, 0.6453, 0.9576, 0.7153, 0.1923, 0.8122, 0.0937, 0.5744,\n",
        "         0.5951, 0.8890, 0.4838, 0.5707, 0.6760, 0.3738, 0.2796, 0.1549,\n",
        "         0.8220, 0.2800, 0.4051, 0.2553, 0.1831, 0.0046, 0.9021, 0.0264,\n",
        "         0.2327, 0.8261, 0.0534, 0.1582, 0.4087, 0.9047, 0.1409, 0.6864,\n",
        "         0.1439, 0.3432, 0.1072, 0.5907, 0.6756, 0.6942, 0.6814, 0.3368,\n",
        "         0.4138, 0.8030, 0.7024, 0.3309, 0.7288, 0.2193, 0.1954, 0.9948,\n",
        "         0.1201, 0.9483, 0.7407, 0.4849, 0.6500, 0.8649, 0.7405, 0.4725,\n",
        "         0.5373, 0.6541, 0.5444, 0.7425, 0.8940, 0.3580, 0.3905, 0.8924,\n",
        "         0.2995, 0.3726, 0.5399, 0.3057, 0.3380, 0.8313, 0.1137, 0.0120,\n",
        "         0.7714, 0.2561, 0.2569, 0.2994, 0.7648, 0.2413, 0.6101\n",
        "        ]])\n",
        "\n",
        "\n",
        "img_shape = (CHANNELS, IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Device:{}'.format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6a0f6381",
      "metadata": {
        "id": "6a0f6381"
      },
      "outputs": [],
      "source": [
        " # %%helper functions\n",
        "\n",
        "def set_all_seeds(seed):\n",
        "    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    \n",
        "def set_deterministic():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "    torch.set_deterministic(True)\n",
        "\n",
        "def plot():\n",
        "    for i,(image, _) in example_loader:\n",
        "        f, axarr = plt.subplots(2)\n",
        "    \n",
        "        # Reshape the array for plotting\n",
        "        axarr[0].imshow(image[0].to(device))\n",
        "    \n",
        "\n",
        "        result = model.decoder(torch.tensor([-0.0,0.03]).to(device))\n",
        "        result = result.squeeze(0)\n",
        "        result = result.squeeze(0)\n",
        "        axarr[1].imshow(result[0].to('cpu').numpy())\n",
        "                  \n",
        "def add_noise(inputs,variance):\n",
        "    noise = torch.randn_like(inputs)\n",
        "    return inputs + variance*noise\n",
        "\n",
        "\n",
        "def save_checkpoint(state, filename):\n",
        "    print(\"=> Saving chekpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint):\n",
        "    model.load_state_dict(checkpoint[STATE_DICT])\n",
        "    optimizer.load_state_dict(checkpoint[MODEL_OPTIMIZER])\n",
        "    #losses = checkpoint[LOSSES]\n",
        "    #recon_losses = checkpoint[RECON_LOSS]\n",
        "    #l_losses = checkpoint[KL_DIV]\n",
        "    #return losses, kl_losses, recon_losses\n",
        "    \n",
        "    \n",
        "def get_numbered_images():  \n",
        "    numberred_images = []\n",
        "    \n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        for i in range(len(labels[:])):\n",
        "            if batch_idx == labels[i].item():\n",
        "                numberred_images.append(images[i])\n",
        "                break          \n",
        "    \n",
        "    return numberred_images\n",
        "          \n",
        "\n",
        "def plot_generated_images(c, figsize=(20, 2.5), n_images=10):\n",
        "    model.to(device)\n",
        "    c = torch.tensor(c, dtype=torch.int64).to(device)\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=n_images, \n",
        "                             sharex=True, sharey=True, figsize=figsize)\n",
        "    \n",
        "    for batch_idx, (images, _) in enumerate(train_loader):\n",
        "        \n",
        "        images = images.to(device)\n",
        "        with torch.no_grad():\n",
        "           encoded, z_mean, z_log_var, decoded_images = model(images,c)[:n_images]\n",
        "\n",
        "        orig_images = images[:n_images]\n",
        "        break\n",
        "\n",
        "    for i in range(n_images):\n",
        "        for ax, img in zip(axes, [orig_images, decoded_images]):\n",
        "            curr_img = img[i].detach().to(torch.device('cpu'))\n",
        "            ax[i].imshow(curr_img.view((28, 28)))\n",
        "            \n",
        "            \n",
        "def plot_numberred_images(numbered_images,figsize=(20, 2.5)):\n",
        "\n",
        "    with torch.no_grad(): \n",
        "        fig, axes = plt.subplots(nrows=2, ncols=10, \n",
        "                                 sharex=True, sharey=True, figsize=(20, 2.5))\n",
        "\n",
        "        for i in range(10):\n",
        "            latent = torch.rand_like(torch.Tensor(20)).to(device)\n",
        "            c = torch.tensor(i, dtype=torch.int64).to(device) \n",
        "            \n",
        "            gen_img = model.decoder(latent,c).detach().to(torch.device('cpu'))\n",
        "            axes[0][i].imshow(numbered_images[i].view((28, 28))) \n",
        "            axes[1][i].imshow(gen_img.view((28, 28)))\n",
        "        \n",
        "        plt.figure().clear()\n",
        "            \n",
        "def plot_image(c):\n",
        "    \n",
        "\n",
        "    with torch.no_grad():\n",
        "        fig, axes = plt.subplots(nrows=1, ncols=1, sharex=True, sharey=True)\n",
        "        \n",
        "        c = torch.tensor(c, dtype=torch.int64).to(device)\n",
        "        latent = torch.rand_like(torch.Tensor(20)).to(device)\n",
        "    \n",
        "        decoded = model.decoder(latent,c).detach().to(torch.device('cpu'))\n",
        "        axes.imshow(decoded.view((28, 28)))   \n",
        "        \n",
        "#plot a grid of r,c images,reccomended with 10,10\n",
        "def plot_many_images(r=10,c=10):\n",
        "    fig, axes = plt.subplots(nrows=r, ncols=c,figsize=(20, 20), sharex=True, sharey=True)\n",
        "    \n",
        "    for i in range(r):\n",
        "        for y in range(c):\n",
        "            caption = torch.tensor(y, dtype=torch.int64).to(device)\n",
        "            latent = torch.rand_like(torch.Tensor(LATENT_SPACE_SIZE)).to(device)\n",
        "    \n",
        "            decoded = model.decoder(latent,caption).detach().to(torch.device('cpu'))\n",
        "            axes[i][y].imshow(decoded.view((28, 28)))   \n",
        "    \n",
        "    plt.figure().clear()\n",
        "\n",
        "\n",
        "def plot_latent_space_with_labels(iteration, num_classes=10):\n",
        "    d = {i:[] for i in range(num_classes)}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (features, targets) in enumerate(train_loader):\n",
        "\n",
        "            features = features.to(device)\n",
        "            targets = targets.to(device)\n",
        "            \n",
        "            embedding = model.encoding_fn(features)\n",
        "\n",
        "            for i in range(num_classes):\n",
        "                if i in targets:\n",
        "                    mask = targets == i\n",
        "                    d[i].append(embedding[mask].to('cpu').numpy())\n",
        "\n",
        "    colors = list(mcolors.TABLEAU_COLORS.items())\n",
        "    for i in range(num_classes):\n",
        "        d[i] = np.concatenate(d[i])\n",
        "        plt.scatter(\n",
        "            d[i][:, 0], d[i][:, 1],\n",
        "            #color=colors[i][1],\n",
        "            #label=f'{i}',\n",
        "            alpha=0.5)\n",
        "\n",
        "\n",
        "    plt.figure().clear()\n",
        "\n",
        "\n",
        "def plot_losses():\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.title(\"Loss During Training\")\n",
        "    plt.plot(losses[:], label=\"L\")\n",
        "    plt.plot(kl_losses[:], label=\"KL\")\n",
        "    plt.plot(recon_losses[:], label=\"Recon\")\n",
        "    plt.xlabel(\"iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    \n",
        "    plt.figure().clear()\n",
        "    \n",
        "def clear_cache():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    \n",
        "\n",
        "\n",
        "def showExample():\n",
        "  for image, _ in example_loader:\n",
        "      f, axarr = plt.subplots(2)\n",
        "      image = image.reshape(-1,28*28).to(device)\n",
        "\n",
        "      model.to(device)\n",
        "      recon = model(image)\n",
        "\n",
        "      image = image.reshape(-1, 28, 28)\n",
        "      axarr[0].imshow(image[0].cpu())\n",
        "\n",
        "\n",
        "      recon = recon.reshape(-1, 28, 28).to('cpu')\n",
        "      axarr[1].imshow(recon[0].detach().numpy())\n",
        "\n",
        "      break        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fa1439af",
      "metadata": {
        "id": "fa1439af"
      },
      "outputs": [],
      "source": [
        "# %%Train Data\n",
        "\n",
        "set_deterministic\n",
        "set_all_seeds(RANDOM_SEED)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"~/torch_datasets\", train=True, transform=transform, download=True\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"~/torch_datasets\", train=False, transform=transform, download=True\n",
        ")\n",
        "\n",
        "train_loader = data.DataLoader(\n",
        "                                train_dataset,\n",
        "                                batch_size=BATCH_SIZE,\n",
        "                                shuffle=SHUFFLE,\n",
        "                                num_workers=NUM_WORKERS,\n",
        "                                pin_memory=PIN_MEMORY,\n",
        "                                drop_last=False\n",
        "                                )\n",
        "\n",
        "test_loader = data.DataLoader(\n",
        "                                test_dataset,\n",
        "                                batch_size=32,\n",
        "                                shuffle=True,\n",
        "                                num_workers=0\n",
        "                                )\n",
        "\n",
        "example_loader = data.DataLoader(\n",
        "                                train_dataset,\n",
        "                                batch_size=1,\n",
        "                                shuffle=True,\n",
        "                                num_workers=0,\n",
        "                                drop_last=True\n",
        "                                 )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "1283e874",
      "metadata": {
        "id": "1283e874"
      },
      "outputs": [],
      "source": [
        "# %%Model\n",
        "\n",
        "class Reshape(nn.Module):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__()\n",
        "        self.shape = args\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.view(self.shape)\n",
        "\n",
        "\n",
        "class Trim(nn.Module):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, :, :28, :28]\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "           \n",
        "        self.z_mean = torch.nn.Linear(3136, LATENT_SPACE_SIZE) # 2 dim for visualization purposes\n",
        "        self.z_log_var = torch.nn.Linear(3136, LATENT_SPACE_SIZE)\n",
        "        \n",
        "        self.d_emb = nn.Embedding(10, 50)\n",
        "        self.e_emb_fc = nn.Linear(50, 784)\n",
        "        self.d_emb_fc = nn.Linear(50, 49)\n",
        "       \n",
        "        #########\n",
        "        # Encoder\n",
        "        #########\n",
        "        self.e_conv1 = nn.Conv2d(1, 32, stride=(1, 1), kernel_size=(3, 3), padding=1)\n",
        "        self.e_conv2 = nn.Conv2d(32, 64, stride=(2, 2), kernel_size=(3, 3), padding=1)\n",
        "        self.e_conv3 = nn.Conv2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1)\n",
        "        self.e_conv4 = nn.Conv2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1)\n",
        "        self.flatten = nn.Flatten()\n",
        "        \n",
        "\n",
        "        #########\n",
        "        # Decoder\n",
        "        #########  \n",
        "        self.d_lin = torch.nn.Linear(LATENT_SPACE_SIZE, 3087)\n",
        "        self.d_conv1 = nn.ConvTranspose2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1)\n",
        "        self.d_conv2 = nn.ConvTranspose2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1)              \n",
        "        self.d_conv3 = nn.ConvTranspose2d(64, 32, stride=(2, 2), kernel_size=(3, 3), padding=0)              \n",
        "        self.d_conv4 = nn.ConvTranspose2d(32, 1, stride=(1, 1), kernel_size=(3, 3), padding=0)\n",
        "       \n",
        "        \n",
        "    def encoder(self, x, c):\n",
        "        #c = self.d_emb(c)\n",
        "        #c = self.e_emb_fc(c)\n",
        "        #c = c.view(-1, 1, 28, 28)\n",
        "        #x = torch.cat((x, c), 1)\n",
        "\n",
        "        x = F.leaky_relu(self.e_conv1(x))\n",
        "        x = F.leaky_relu(self.e_conv2(x))\n",
        "        x = F.leaky_relu(self.e_conv3(x))\n",
        "        x = self.e_conv4(x)\n",
        "        x = self.flatten(x)\n",
        "        x = x.view(-1)\n",
        "        return x\n",
        "    \n",
        "    \n",
        "    def decoder(self, x, c):\n",
        "        c = self.d_emb(c)\n",
        "        c = self.d_emb_fc(c)\n",
        "        c = c.view(-1, 1, 7, 7)\n",
        "        \n",
        "        x = self.d_lin(x)\n",
        "        x = x.view(-1, 63, 7, 7)\n",
        "        \n",
        "        x = torch.cat((c, x), 1)\n",
        "        \n",
        "        x = x.view(-1,64,7,7)\n",
        "        x = F.leaky_relu(self.d_conv1(x))\n",
        "        x = F.leaky_relu(self.d_conv2(x))\n",
        "        x = F.leaky_relu(self.d_conv3(x))\n",
        "        x = F.leaky_relu(self.d_conv4(x))\n",
        "        x = x[:, :, :28, :28]\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n",
        "    \n",
        "    def encoding_fn(self, x, c):\n",
        "        x = self.encoder(x, c)\n",
        "        z_mean, z_log_var = self.z_mean(x), self.z_log_var(x)\n",
        "        encoded = self.reparameterize(z_mean, z_log_var)\n",
        "        return encoded\n",
        "        \n",
        "    def reparam(self,mu,log_var):\n",
        "        std = torch.exp(0.5*log_var)\n",
        "        eps = torch.rand_like(std)\n",
        "        z = mu+ eps*std\n",
        "\n",
        "    def reparameterize(self, z_mu, z_log_var):\n",
        "        std = torch.exp(0.5*z_log_var)\n",
        "        eps = torch.rand_like(std).to(device)\n",
        "        z = z_mu + eps * std\n",
        "        return z\n",
        "        \n",
        "    def forward(self, x, c):\n",
        "        x = self.encoder(x, c)\n",
        "        z_mean, z_log_var = self.z_mean(x), self.z_log_var(x)\n",
        "        encoded = self.reparameterize(z_mean, z_log_var) # sample μ,σ to create distribution\n",
        "        decoded = self.decoder(encoded,c)\n",
        "        return encoded, z_mean, z_log_var, decoded\n",
        " \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "d91435f7",
      "metadata": {
        "id": "d91435f7"
      },
      "outputs": [],
      "source": [
        "# %% Instantiate model, loss func, optimizer\n",
        "\n",
        "model = VAE()\n",
        " \n",
        "# Validation using MSE Loss function\n",
        "loss_function = nn.MSELoss(reduction='none')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, betas=(B1 ,B2))\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    loss_function = loss_function.cuda()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "69e678ef",
      "metadata": {
        "id": "69e678ef",
        "outputId": "270b0e49-1d30-4fa6-cad8-9931ec4a0149",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2000, 3136])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-e9800edd57e4>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#st = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannels_last\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-9aafd94165fc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, c)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_log_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_log_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-9aafd94165fc>\u001b[0m in \u001b[0;36mreparameterize\u001b[0;34m(self, z_mu, z_log_var)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_log_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz_mu\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_log_var\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Device index must not be negative"
          ]
        }
      ],
      "source": [
        "# %%Train Model\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to(memory_format=torch.channels_last)\n",
        "    loss_function.to(device)\n",
        "    model.train()\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "  \n",
        "\n",
        "logging_interval = 10\n",
        "losses = []\n",
        "kl_losses = []\n",
        "recon_losses = []\n",
        "recon_loss = 0\n",
        "kl_div = 0\n",
        "iter = 0\n",
        "alpha = 1\n",
        "\n",
        "\n",
        "    \n",
        "for epoch in range(4):\n",
        "    #st = time.time()\n",
        "    for batch, (imgs, labels) in enumerate(train_loader):\n",
        "\n",
        "        imgs = imgs.to(device,memory_format=torch.channels_last)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # set gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "        encoded, z_mean, z_log_var, decoded = model(imgs,labels)\n",
        "        print(\"imgs: \",imgs.shape)\n",
        "        print(\"labels: \",labels.shape)\n",
        "        \n",
        "\n",
        "        # Total Loss = Recon_loss + KLDivergence\n",
        "        kl_div = -0.5 * torch.sum(1 + z_log_var \n",
        "                                  - z_mean**2 \n",
        "                                  - torch.exp(z_log_var), \n",
        "                                  axis=1) # sum over latent dimension\n",
        "\n",
        "        kl_div = kl_div.mean()\n",
        "\n",
        "        recon_loss = loss_function(decoded, imgs)\n",
        "        recon_loss = recon_loss.view(BATCH_SIZE, -1).sum(axis=1) # sum over pixels\n",
        "        recon_loss = recon_loss.mean() # average over batch dimension\n",
        "        \n",
        "        loss = alpha*recon_loss + kl_div\n",
        "        loss.backward()\n",
        "        optimizer.step()  # Updates Weights\n",
        "        \n",
        "        loss_item = loss.item()\n",
        "        recon_loss_item = recon_loss.item()\n",
        "        kl_div_item = kl_div.item()\n",
        "        \n",
        "        #scheduler.step()\n",
        "        \n",
        "        \n",
        "        if iter % logging_interval == 0:\n",
        "            print('[%d/%d][%d/%d]\\t, LOSS:%.4f (recon_loss : %.4f, kl_loss = %.6f'\n",
        "                  %(epoch, NUM_EPOCHS, batch, len(train_loader), loss_item, recon_loss_item, kl_div_item))\n",
        "        \n",
        "        \n",
        "        losses.append(loss_item)\n",
        "        kl_losses.append(kl_div_item)\n",
        "        recon_losses.append(recon_loss_item)\n",
        "        \n",
        "        iter +=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "511a3c3e",
      "metadata": {
        "id": "511a3c3e"
      },
      "outputs": [],
      "source": [
        "# %% Save Model\n",
        "checkpoint = {STATE_DICT : model.state_dict(),\n",
        "              MODEL_OPTIMIZER : optimizer.state_dict(),\n",
        "              LOSSES: losses,\n",
        "              RECON_LOSS:recon_losses,\n",
        "              KL_DIV:kl_losses}\n",
        "save_checkpoint(checkpoint, \"cVAE4.pth.tar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "55898553",
      "metadata": {
        "id": "55898553"
      },
      "outputs": [],
      "source": [
        "# %%  Load Model\n",
        "load_checkpoint(torch.load(\"cVAE4.pth.tar\",map_location=('cpu')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "b8f0ec35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "b8f0ec35",
        "outputId": "cff8cbc9-0d42-4ddc-aba5-66efbbc964d4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 3 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJwAAAGgCAYAAABfdw4vAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAca0lEQVR4nO3dbUxUWZ4G8KequngRi6ILh8JqqJGZ2KMbe3UXBWldVw0rQyeujOxEN5uIrhm3pwsTJBMTJoqRmLDxi8QJSrJxIH4gGj6IabubnQ5249grOLJhNk63rM4wbU0j1dq9FFDKS1FnP9AUngLBklun3p5fcpM6dW/JsXg4dW7de/9XJ4QQIFJEH+4OUHxh4EgpBo6UYuBIKQaOlGLgSCkGjpRi4EgpBo6UYuBIqZAFrr6+HitWrEBSUhLy8/Nx+/btUP0oiiK6UBxLvXz5Mvbt24eGhgbk5+ejrq4OLS0t6O3tRUZGxryv9fl86O/vh8lkgk6n07prFAJCCAwPD8Nms0GvX2AMEyGQl5cnHA6Hvz05OSlsNpuora1d8LVOp1MA4BKFi9PpXPD3+9pcIVyM8fFxdHd3o6qqyv+cXq9HYWEhbt26NWv7sbExjI2N+dviuwF3M97BazBq3T0KAS8mcBMfwmQyLbit5oF78uQJJicnYbVapeetVivu3bs3a/va2lqcPHlyjo4Z8ZqOgYsK303KXmYKFPa91KqqKrjdbv/idDrD3SUKIc1HuGXLlsFgMMDlcknPu1wuZGZmzto+MTERiYmJWneDIpTmI1xCQgJyc3PR3t7uf87n86G9vR0FBQVa/ziKMpqPcABQWVmJsrIyrF+/Hnl5eairq4PH48GBAwdC8eMoioQkcHv27MHjx49RXV2NgYEBrFu3Dm1tbbN2JCj+hOSL38UYGhqC2WzGVuziXmqU8IoJfIqrcLvdSE1NnXfbsO+lUnxh4EgpBo6UYuBIKQaOlGLgSCkGjpRi4EgpBo6UCsmhLZrxqPJt/+P/+cU5ad36Ez+X2un/MfsE1VjDEY6UYuBIKQaOlOIcLsSeWWdOxpkUPmnd00z5GoB0JT0KL45wpBQDR0oxcKQUA0dKMXCkFANHSjFwpBQDR0oxcKQUA0dKMXCkFANHSjFwpBQDR0oFHbgbN25g586dsNls0Ol0aG1tldYLIVBdXY3ly5cjOTkZhYWFuH//vlb9pSgXdOA8Hg/Wrl2L+vr6OdefPn0aZ8+eRUNDA7q6upCSkoKioiKMjo4uurMU/YI+AbO4uBjFxcVzrhNCoK6uDseOHcOuXbsAABcvXoTVakVrayv27t076zWBVcyHhoaC7RJFEU3ncH19fRgYGEBhYaH/ObPZjPz8/DlL5gNTVczNZrN/yc7O1rJLFGE0DdzAwAAAzFkyf3pdIFYxjy9hv6aBVczji6Yj3HRZ/JctmU/xR9PA5eTkIDMzUyqZPzQ0hK6uLpbMJwCv8JE6MjKCBw8e+Nt9fX3o6emBxWKB3W5HRUUFTp06hZUrVyInJwfHjx+HzWZDSUmJlv2mKBV04O7cuYNt27b525WVlQCAsrIyNDU14ejRo/B4PDh06BAGBwexefNmtLW1ISkpSbteRxGfMaKKxIdd0IHbunUr5qu0r9PpUFNTg5qamkV1jGITj6WSUgwcKRX27+Fi3T9t63zhujduPFPYk8jAEY6UYuBIKX6kakwXcJhumfHJC7c1PpLPjJkMSY8iC0c4UoqBI6UYOFKKcziN6VfIJ5BWvh77pfCDwRGOlGLgSCkGjpTiHE6h2m/+Smr7/vRlmHoSPhzhSCkGjpRi4EgpzuEUcnuTpbbwesPUk/DhCEdKMXCkFANHSnEOpzXXY6nZMjJzU8pbX+dI61LwJyVdiiQc4UgpBo6UYuBIKc7htGb9ntT86dJv/I+P98h1837AORxRaDFwpFRQgautrcWGDRtgMpmQkZGBkpIS9Pb2StuMjo7C4XAgPT0dS5cuRWlp6awChRS/ggpcR0cHHA4HOjs78fHHH2NiYgI7duyAx+Pxb3PkyBG8//77aGlpQUdHB/r7+7F7927NOx6pHv1DhrQ877VnOmmJR0HtNLS1tUntpqYmZGRkoLu7G1u2bIHb7caFCxfQ3NyM7du3AwAaGxuxevVqdHZ2YuPGjbP+TZbNjy+LmsO53W4AgMViAQB0d3djYmJCKpu/atUq2O12ls0nAIsInM/nQ0VFBTZt2oQ1a9YAmCqbn5CQgLS0NGlbls2naa/8PZzD4cDdu3dx8+bNRXUg1srmj1pevO77Hw5L7XgsxvpKI1x5eTmuXbuGTz75BFlZWf7nMzMzMT4+jsHBQWl7ls2naUEFTgiB8vJyXLlyBdevX0dOjnz2Q25uLoxGo1Q2v7e3Fw8fPmTZfAIQ5Eeqw+FAc3Mzrl69CpPJ5J+Xmc1mJCcnw2w24+DBg6isrITFYkFqaioOHz6MgoKCOfdQKf4EFbjz588DmKpk/rzGxkbs378fAHDmzBno9XqUlpZibGwMRUVFOHfunCadjUadM9/4wND/jbQu/q5oCDJw85XLn5aUlIT6+voX3k+V4huPpZJSDBwpxfPhQuzw3X/2P/7eV73zbBkfOMKRUgwcKcWP1BAb7UpfeKM4whGOlGLgSCkGjpTiHE5j9pr/CncXIhpHOFKKgSOlGDhSioEjpRg4UoqBI6UYOFKKgSOlGDhSioEjpSLu0Nb0hTpeTMTnpelRyIsJAC93kVXEBW54eKocwk18GOaeULCGh4dhNpvn3UYnXiaWCvl8PvT390MIAbvdDqfTidTU1HB3K2oMDQ0hOztb6fsmhMDw8DBsNhv0+vlnaRE3wun1emRlZfnrxKWmpjJwr0D1+7bQyDaNOw2kFANHSkVs4BITE3HixImYqh2nQqS/bxG300CxLWJHOIpNDBwpxcCRUgwcKcXAkVIMHCnFwJFSDBwpxcCRUgwcKcXAkVIMHCnFwJFSIQtcfX09VqxYgaSkJOTn5+P27duh+lEURUJyetLly5exb98+NDQ0ID8/H3V1dWhpaUFvby8yMjLmfe30NQ0mkwk6XXzeFz7aBHNNA0QI5OXlCYfD4W9PTk4Km80mamtrF3yt0+kUmLpAkEuULU6nc8Hfr+YX0YyPj6O7uxtVVVX+5/R6PQoLC+e87/3Y2BjGxmZuuSe+G3D/zvCPeE1nnHrOG6P33QscwaP0XFgvJnATH8JkMi24reaBe/LkCSYnJ2G1WqXnrVYr7t27N2v72tpanDx5cnbHdMaZwMXqR+us/1d0Bm662y8zBQr7XmpVVRXcbrd/cTqd4e4ShZDmI9yyZctgMBjgcrmk51903/vExMQ5L/gQXm/sjmzTovQjdDE0H+ESEhKQm5sr3ffe5/Ohvb2d972n0Fx5X1lZibKyMqxfvx55eXmoq6uDx+PBgQMHQvHjKIqEJHB79uzB48ePUV1djYGBAaxbtw5tbW2zdiQo/kTcdalDQ0Mwm83Yil3+vVSKbF4xgU9xFW63e8F6JmHfS6X4wsCRUgwcKRVx9eHiSuD3jLqAv3/hC2hH1HT7lXCEI6UYOFKKgSOlOIdTSBdwzPibf/lbqT2SLc/pEgbl17/xkXx8evJ//6hZ31ThCEdKMXCkFD9SVXprpdT85u/HpHbLlgapfeD3ZVLb86d0qZ3Ej1Si+TFwpBQDR0pxDqfQ06wlUrv0rd9J7STdpNTe/Eaf1P5v8zp5e+26pgxHOFKKgSOlGDhSinO4UNMb/A+fWQzSqk2m+1I7K+C3YTF6pPaSr6O/AgFHOFKKgSOlGDhSinO4ENMZZuZt42b59COLYURqGyCv/81Xq6T29+4/ltrROKPjCEdKMXCkFANHSnEOF2I6w8zftDdZXpeiG5faX3rlywC//UI+/y39UY+mfQsHjnCkVNCBu3HjBnbu3AmbzQadTofW1lZpvRAC1dXVWL58OZKTk1FYWIj79+/P/Y9R3Ak6cB6PB2vXrkV9ff2c60+fPo2zZ8+ioaEBXV1dSElJQVFREUZHRxfdWYp+Qc/hiouLUVxcPOc6IQTq6upw7Ngx7Nq1CwBw8eJFWK1WtLa2Yu/evYvrbRTSpcycAzdmkUs3mPQTUntgMkVqJ34rjwe+MfkaiGik6Ryur68PAwMDKCws9D9nNpuRn58/Z8l8YKps/tDQkLRQ7NI0cAMDAwAwZ8n86XWBamtrYTab/Ut2draWXaIIE/a9VJbNjy+afg83XRbf5XJh+fLl/uddLhfWrVs352teVDY/VvhWzLwP7/34N9I6S8Cf+3+OfF9qpz1gua555eTkIDMzUyqZPzQ0hK6uLpbMJwCvMMKNjIzgwYMH/nZfXx96enpgsVhgt9tRUVGBU6dOYeXKlcjJycHx48dhs9lQUlKiZb8pSgUduDt37mDbtm3+dmVlJQCgrKwMTU1NOHr0KDweDw4dOoTBwUFs3rwZbW1tSEqKxovaSGtBB27r1q2Yr9K+TqdDTU0NampqFtWxWPHtWzNl5N9eIh9xMQaUWP3zqHzs1Pz7J1Jbvmo1OoV9L5XiCwNHSjFwpBTPh9OY7jX5LR187rKEHxjlExh8Qr6GofVGntR+88seTfsWCTjCkVIMHCnFj1SN6U0mqZ29/iv/43S9fI75H73PpLapL+B0pBg8h5AjHCnFwJFSDBwpxTmc1mwZUvPf7G3+x4aAQ1lOr3z3ZNvHcimHWDiUFYgjHCnFwJFSDBwpxTmcxlybLFL7TePXz7XkU+l/cfenUjvzy4eh6lbE4AhHSjFwpBQDR0pxDrdI+oBrNdxvyqffWw0z5RzcPvmyv5EvXpfavqf3NO5d5OEIR0oxcKQUA0dKcQ63WD/KkZpZfy0X7Vny3K2Pbo2mSesybweUcogDHOFIKQaOlGLgSCnO4RZp5IfyOW0VKz6Q2k99M2e1Xfu/ddK61Nt/kdrReCujYHGEI6WCClxtbS02bNgAk8mEjIwMlJSUoLe3V9pmdHQUDocD6enpWLp0KUpLS+FyuTTtNEWvoALX0dEBh8OBzs5OfPzxx5iYmMCOHTvg8czcufjIkSN4//330dLSgo6ODvT392P37t2ad5yiU1BzuLa2Nqnd1NSEjIwMdHd3Y8uWLXC73bhw4QKam5uxfft2AEBjYyNWr16Nzs5ObNy4Ubueh4tevo344A/l9qqEF4/mH/32b6T2m096NOtWtFjUHM7tdgMALJapkw67u7sxMTEhlc1ftWoV7HY7y+YTgEUEzufzoaKiAps2bcKaNWsATJXNT0hIQFpamrQty+bTtFcOnMPhwN27d3Hp0qVFdYBl8+PLK30PV15ejmvXruHGjRvIysryP5+ZmYnx8XEMDg5Ko5zL5fKX1A8UbWXzDenyNQvPrIG3M5KvJh1+riSXcVj++xbeePjmTRbUCCeEQHl5Oa5cuYLr168jJ0c+cJ2bmwuj0SiVze/t7cXDhw9ZNp8ABDnCORwONDc34+rVqzCZTP55mdlsRnJyMsxmMw4ePIjKykpYLBakpqbi8OHDKCgoiI09VFq0oAJ3/vx5AFOVzJ/X2NiI/fv3AwDOnDkDvV6P0tJSjI2NoaioCOfOndOksxT9ggrcfOXypyUlJaG+vv6F91ONemly/TcR8A4GzlGerx9iHJbXcQ5HFGIMHCnFwJFSPB8uSLqRp1Lb8FQuff8Xr1zH99///I7/se1GwCQuDnGEI6UYOFKKH6lB8rrksqi2z+STDf7Vc1hqG2dOFUTmna6Q9StacIQjpRg4UoqBI6U4hwuWTz79KPGD30nt7I/kU84hnjt96SUODcY6jnCkFANHSjFwpBTncFrzxeINi7TDEY6UYuBIKQaOlGLgSCkGjpRi4EipiPtaZPrKMC8mAB4JigpeTN1t52Wu6ou4wA0PT52GfRMfhrknFKzh4WGYzeZ5t9GJl4mlQj6fD/39/RBCwG63w+l0IjU1deEXEgBgaGgI2dnZSt83IQSGh4dhs9mg188/S4u4EU6v1yMrK8tfJy41NZWBewWq37eFRrZp3GkgpRg4UipiA5eYmIgTJ05EVe24SBDp71vE7TRQbIvYEY5iEwNHSjFwpBQDR0oxcKQUA0dKMXCkFANHSjFwpBQDR0oxcKQUA0dKMXCkVMgCV19fjxUrViApKQn5+fm4fft2qH4URZGQnJ50+fJl7Nu3Dw0NDcjPz0ddXR1aWlrQ29uLjIyMeV87fU2DyWSCTqebd1uKDMFc0wARAnl5ecLhcPjbk5OTwmazidra2gVf63Q6BaYuEOQSZYvT6Vzw96v5RTTj4+Po7u5GVVWV/zm9Xo/CwkLcunVr1vZjY2MYGxvzt8V3A+7fGf4Rr+mMU8/F6l33AkfwKD0X1osJ3MSHMJlMC26reeCePHmCyclJWK1W6Xmr1Yp79+7N2r62thYnT56c3TGdcSZwsfrROuv/FZ2Bm+72y0yBwr6XWlVVBbfb7V+cTieAqVFteolZQshLHNB8hFu2bBkMBgNcLpf0vMvlQmZm5qztExMTI/aCD9Ke5iNcQkICcnNz0d7e7n/O5/Ohvb0dBQUFWv84ijIhufK+srISZWVlWL9+PfLy8lBXVwePx4MDBw6E4sdRFAlJ4Pbs2YPHjx+juroaAwMDWLduHdra2mbtSFD8ibjrUoeGhmA2m7EVu/x7qRTZvGICn+Iq3G73gvVMwr6XSvGFgSOlIq5cV1wJ/KJUF/D3//yN4YCY+K6OIxwpxcCRUgwcKcU5nEK6gEN43/zL30rtkWx5TpcwKL/+jY/kw4WT//tHzfqmCkc4UoqBI6X4karSWyul5jd/Pya1W7Y0SO0Dvy+T2p4/pUvtJH6kEs2PgSOlGDhSinM4hZ5mLZHapW/9Tmon6Sal9uY3+qT2f5vXydtr1zVlOMKRUgwcKcXAkVKcw4Wa3uB/+MxikFZtMt2X2lkBvw2L0SO1l3wd/ZdMcoQjpRg4UoqBI6U4hwsxnWFm3jZulk8/shhGpLYB8vrffLVKan/v/mOpHY0zOo5wpBQDR0oxcKQU53AhpjPM/E17k+V1Kbpxqf2lV74M8Nsv5PPf0h/1aNq3cOAIR0oxcKRU0IG7ceMGdu7cCZvNBp1Oh9bWVmm9EALV1dVYvnw5kpOTUVhYiPv378/9j1HcCTpwHo8Ha9euRX19/ZzrT58+jbNnz6KhoQFdXV1ISUlBUVERRkdHF93ZaKRLWeJfxiw+aTHpJ6TlG1+ytCR+q5cW39iYtESjoHcaiouLUVxcPOc6IQTq6upw7Ngx7Nq1CwBw8eJFWK1WtLa2Yu/evbNeE1jFfGhoKNguURTRdA7X19eHgYEBFBYW+p8zm83Iz8+fs2Q+MFXF3Gw2+5fs7Gwtu0QRRtPADQwMAMCcJfOn1wV6URVzik1h/x4u1quY+1Ys9z9+78e/kdZZAv7c/3Pk+1I77QHLdc1ruiz+y5bMp/ijaeBycnKQmZkplcwfGhpCV1cXS+YTgFf4SB0ZGcGDBw/87b6+PvT09MBiscBut6OiogKnTp3CypUrkZOTg+PHj8Nms6GkpETLflOUCjpwd+7cwbZt2/ztyspKAEBZWRmamppw9OhReDweHDp0CIODg9i8eTPa2tqQlBSNV1Eu3rdvzVT1fnuJ/AW4MaDE6p9H5WOn5t8/kdryVavRKejAbd26FfNV2tfpdKipqUFNTc2iOkaxicdSSSkGjpQK+/dwsUb3mvyWDj53WcIPjPLxZJ+Qr2FovZEntd/8skfTvkUCjnCkFANHSvEjVWP6gPu+Z6//yv84XS+fY/5H7zOpbeqT//59MXhKF0c4UoqBI6UYOFKKczit2TKk5r/Z2/yPDQGHspxe+Wa2to/lUg6xcCgrEEc4UoqBI6UYOFKKcziNuTZZpPabxq+fa8mn0v/i7k+lduaXD0PVrYjBEY6UYuBIKQaOlOIcbpH0AafOu9+Uz4a2GiZm1vnky/5Gvnhdavue3tO4d5GHIxwpxcCRUgwcKcU53GL9KEdqZv21XENlyXO3Pro1miaty7wdUMohDnCEI6UYOFKKgSOlOIdbpJEfyue0Vaz4QGo/9c2c1Xbt/9ZJ61Jv/0VqR+OtjILFEY6UYuBIqaACV1tbiw0bNsBkMiEjIwMlJSXo7e2VthkdHYXD4UB6ejqWLl2K0tLSWQUKKX4FNYfr6OiAw+HAhg0b4PV68ctf/hI7duzA559/jpSUFADAkSNH8MEHH6ClpQVmsxnl5eXYvXs3Pvvss5D8B5TTy7cRH/yh3F6V8OI/ro9++zdS+80nPZp1K1oEFbi2tjap3dTUhIyMDHR3d2PLli1wu924cOECmpubsX37dgBAY2MjVq9ejc7OTmzcuHHWv8my+fFlUXM4t9sNALBYps5y7e7uxsTEhFQ2f9WqVbDb7SybTwAWETifz4eKigps2rQJa9asATBVNj8hIQFpaWnStiybT9Ne+Xs4h8OBu3fv4ubNm4vqQLSVzTeky9csPLPKx0NNevlq0uHnSnIZh+W/b+GNh2/eZK80wpWXl+PatWv45JNPkJWV5X8+MzMT4+PjGBwclLZn2XyaFlTghBAoLy/HlStXcP36deTkyGdK5Obmwmg0SmXze3t78fDhQ5bNJwBBfqQ6HA40Nzfj6tWrMJlM/nmZ2WxGcnIyzGYzDh48iMrKSlgsFqSmpuLw4cMoKCiYcw+V4k9QgTt//jyAqUrmz2tsbMT+/fsBAGfOnIFer0dpaSnGxsZQVFSEc+fOadLZiJAm138TAe9g4EfG8/VDjMPyunicwwUVuPnK5U9LSkpCfX39C++nSvGNx1JJKQaOlOL5cEHSjTyV2oancun7v3jlOr7//ud3/I9tNwImcXGIIxwpxcCRUvxIDZLXJZdFtX0mn2zwr57DUtvomXmceacrZP2KFhzhSCkGjpRi4EgpzuGC5ZNPP0r84HdSO/sj+ZRziOdOX3qJIzWxjiMcKcXAkVIMHCnFOZzWfLF4wyLtcIQjpRg4UoqBI6UYOFKKgSOlGDhSKuK+Fpm+UMeLCYBHgqKCF1N323mZi6wiLnDDw1OnYd/Eh2HuCQVreHgYZrN53m104mViqZDP50N/fz+EELDb7XA6nUhNTV34hQRgqtxZdna20vdNCIHh4WHYbDbo9fPP0iJuhNPr9cjKyvLXiUtNTWXgXoHq922hkW0adxpIKQaOlIrYwCUmJuLEiRNRVTsuEkT6+xZxOw0U2yJ2hKPYxMCRUgwcKcXAkVIMHCkVsYGrr6/HihUrkJSUhPz8fNy+fTvcXYoYUX3PMxGBLl26JBISEsSvf/1r8Yc//EH87Gc/E2lpacLlcoW7axGhqKhINDY2irt374qenh7xzjvvCLvdLkZGRvzbvPvuuyI7O1u0t7eLO3fuiI0bN4q33347jL2eEpGBy8vLEw6Hw9+enJwUNptN1NbWhrFXkevrr78WAERHR4cQQojBwUFhNBpFS0uLf5svvvhCABC3bt0KVzeFEEJE3Efq+Pg4uru7pft16fV6FBYWvvB+XfFOi3ueqRJxgXvy5AkmJydhtVql5+e7X1c80+qeZ6pE3OlJFByt7nmmSsSNcMuWLYPBYJi1R8X7dc0Wjfc8i7jAJSQkIDc3V7pfl8/nQ3t7O+/X9R0Rzfc8C+suywtcunRJJCYmiqamJvH555+LQ4cOibS0NDEwMBDurkWEn//858JsNotPP/1UPHr0yL88ffrUv827774r7Ha7uH79urhz544oKCgQBQUFYez1lIgMnBBC/OpXvxJ2u10kJCSIvLw80dnZGe4uRQxMXc82a2lsbPRv8+zZM/Hee++J119/XSxZskT85Cc/EY8ePQpfp7/D8+FIqYibw1FsY+BIKQaOlGLgSCkGjpRi4EgpBo6UYuBIKQaOlGLgSCkGjpT6f4PK+33t/EMyAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# %% print reconstructed image vs generated image\n",
        "if 'numbered_images' not in locals():\n",
        "    numbered_images = get_numbered_images()\n",
        "\n",
        "torch.device = device\n",
        "model = model.to(device)\n",
        "\n",
        "c = 1\n",
        "with torch.no_grad():\n",
        "    fig, axes = plt.subplots(nrows=3, ncols=1, sharex=True, sharey=True)\n",
        "    #for batch_idx, (images, labels) in enumerate(example_loader):\n",
        "    \n",
        "    c = torch.tensor(c, dtype=torch.int64).to(device)\n",
        "    latent = torch.rand_like(torch.Tensor(20)).to(device)\n",
        "    img = numbered_images[c].to(device)\n",
        "  \n",
        "\n",
        "    \n",
        "    decoded = model.decoder(latent,c).detach().cpu()\n",
        "    #recon = model()\n",
        "    \n",
        "    #image from dataset\n",
        "    axes[0].imshow(numbered_images[c.item()][None, :].view((28, 28)))   \n",
        "    \n",
        "    #image from decoder, caption\n",
        "    axes[1].imshow(decoded.view((28, 28)))  \n",
        "\n",
        "    #reconstructed_image\n",
        "    x = model(img, c)[3]\n",
        "    axes[2].imshow(decoded.view((28, 28)))  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  if 'numbered_images' not in locals():\n",
        "    numbered_images = get_numbered_images()\n",
        "\n",
        "  fig, axes = plt.subplots(nrows=1, ncols=1, sharex=True, sharey=True)\n",
        "  img = numbered_images[1].to(device)\n",
        "  cap = torch.tensor(1, dtype=torch.int64).to(device)\n",
        "\n",
        "\n",
        "  x = model(img, cap)[3]\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  #_,_,_,img = model(img,cap)\n",
        "  axes.imshow(x.view(28, 28))  \n"
      ],
      "metadata": {
        "id": "2ciZ75_4fLsu",
        "outputId": "782a9fb7-e038-4915-9326-10355957b84c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "id": "2ciZ75_4fLsu",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z_log_var.shape"
      ],
      "metadata": {
        "id": "PjuQ0of7nAVx",
        "outputId": "04b5814a-c309-420f-b621-3015dd7eb781",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PjuQ0of7nAVx",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
      }
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}