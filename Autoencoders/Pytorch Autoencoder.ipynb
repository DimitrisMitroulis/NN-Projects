{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder Consists of 3 parts:\n",
    "    1. Encoder\n",
    "    2. Bottleneck\n",
    "    3. Decoder\n",
    "    \n",
    "Encoder/Decoder are fully connected feed foward neural networks\n",
    "and the bottleneck is \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import datasets\n",
    "\n",
    "\n",
    "#torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "tensor_transform = transforms.ToTensor()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% train data\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"~/torch_datasets\", train=True, transform=transform, download=True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"~/torch_datasets\", train=False, transform=transform, download=True\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=256, shuffle=True, num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=32, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "example_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=1, shuffle=True, num_workers=0,drop_last=True,\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    transforms.Resize((300,300))\n",
    "    ])\n",
    "\n",
    "flowers_train_dataset = torchvision.datasets.Flowers102(\n",
    "    root=\"~/torch_datasets\",split=\"train\",transform=flower_transform,download=True)\n",
    "\n",
    "flower_loader = torch.utils.data.DataLoader(\n",
    "    flowers_train_dataset, batch_size=256, shuffle=True, num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "def plot():\n",
    "    f, axarr = plt.subplots(2)\n",
    "\n",
    "    for i, item in enumerate(image):\n",
    "    # Reshape the array for plotting\n",
    "        item = item.reshape(-1, 28, 28)\n",
    "        axarr[0].imshow(item[0].cpu())\n",
    "\n",
    "    for i, item in enumerate(reconstructed):\n",
    "        item = item.reshape(-1, 28, 28).cpu()\n",
    "        item = item.detach().numpy()\n",
    "        axarr[1].imshow(item[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "def showExample():\n",
    "    for image, _ in example_loader:\n",
    "        f, axarr = plt.subplots(2)\n",
    "        image = image.reshape(-1,28*28).to(device)\n",
    "\n",
    "        model.to(device)\n",
    "        recon = model(image)\n",
    "\n",
    "        image = image.reshape(-1, 28, 28)\n",
    "        axarr[0].imshow(image[0].cpu())\n",
    "\n",
    "\n",
    "        recon = recon.reshape(-1, 28, 28).to('cpu')\n",
    "        axarr[1].imshow(recon[0].detach().numpy())\n",
    "\n",
    "        break\n",
    "\n",
    "def add_noise(inputs,variance):\n",
    "    noise = torch.randn_like(inputs)\n",
    "    return inputs + variance*noise\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pin_memory = true allowsto speed up training when we load to cpu then push to gpu \n",
    "(Lets you allocate sampl es in page-locked memory, should be enabled when training on gpu )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets.MNIST()\n",
    "from torchvision import datasets \n",
    "\n",
    "dataset = datasets.MNIST(root = \"./data\",\n",
    "                         train = True,\n",
    "                         download = True,\n",
    "                         transform = tensor_transform)\n",
    "\n",
    "\n",
    " \n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=12,\n",
    "                                     shuffle= True,\n",
    "                                     generator=torch.Generator(device=device))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will be constructing the encoder and decoder, 2 fully connected, feed forward Neural networks  \n",
    "\n",
    "Encoder will gradually reduce dimentionality  \n",
    "28*28=784 -> 128 -> 64 -> 36 -> 18 -> 9\n",
    "  \n",
    "Decoder will do the opposite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28  #784\n",
    "hidden_size = 128\n",
    "code_size = 32\n",
    "\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Encoder \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size,hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size,code_size)\n",
    "        )\n",
    "        \n",
    "        #Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(code_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.model = nn.Sequential(\n",
    "        \n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model Initialization\n",
    "model = autoencoder()\n",
    "model.to(device)\n",
    " \n",
    "# Validation using MSE Loss function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "#Adam Optimizer with lr = 0.1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model for 20 epoch:\n",
    "Things to notice:\n",
    "- Firstly we are setting to zero gradient before each backpropagation\n",
    "    because pytorch accumulates the gradients on subsequent backward losses\n",
    "    (this may be usefull when training RNNs)\n",
    "- then we are passing the image through the model and calculate loss with a simple MSE Loss$$ (x - g(f(x)))^{2} $$\n",
    "\n",
    "- loss.backward() computes loss and we are preforming backpropagation with optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/20, loss = 0.074463\n",
      "epoch : 2/20, loss = 0.061052\n",
      "epoch : 3/20, loss = 0.058871\n",
      "epoch : 4/20, loss = 0.054600\n",
      "epoch : 5/20, loss = 0.051979\n",
      "epoch : 6/20, loss = 0.051799\n",
      "epoch : 7/20, loss = 0.052511\n",
      "epoch : 8/20, loss = 0.050143\n",
      "epoch : 9/20, loss = 0.050615\n",
      "epoch : 10/20, loss = 0.049671\n",
      "epoch : 11/20, loss = 0.049430\n",
      "epoch : 12/20, loss = 0.048494\n",
      "epoch : 13/20, loss = 0.048781\n",
      "epoch : 14/20, loss = 0.048546\n",
      "epoch : 15/20, loss = 0.049482\n",
      "epoch : 16/20, loss = 0.048745\n",
      "epoch : 17/20, loss = 0.047823\n",
      "epoch : 18/20, loss = 0.048055\n",
      "epoch : 19/20, loss = 0.047880\n",
      "epoch : 20/20, loss = 0.047976\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "losses = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for image, _ in train_loader:\n",
    "        image = image.reshape(-1,28*28).to(device)\n",
    "        noised_image = add_noise(image,0.2)\n",
    "        #set gradients to zero\n",
    "        \n",
    "        reconstructed = model(noised_image)\n",
    "        loss = loss_function(reconstructed , noised_image)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # Preforms Backpropagation and calculates  gradients \n",
    "        optimizer.step() # Updates Weights based on the gradients computed above\n",
    "        losses += loss.item()\n",
    "    losses = losses / len(train_loader)\n",
    "    print(\"epoch : {}/{}, loss = {:.6f}\".format(epoch + 1, epochs, loss))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showExample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "for image, _ in train_loader:\n",
    "    #example = torch.movedim(image[0],(1,2),(0,1))\n",
    "   \n",
    "    f, axarr = plt.subplots(2)\n",
    "    image = image.reshape(-1,28*28).to(device)\n",
    "    \n",
    "    image = add_noise(image,0.0)\n",
    "    model.to(device)\n",
    "    recon = model(image)\n",
    "    image = image.reshape(-1, 28, 28)\n",
    "   \n",
    "    axarr[0].imshow(image[0].cpu())\n",
    "    \n",
    "\n",
    "    \n",
    "    recon = recon.reshape(-1, 28, 28).to('cpu')\n",
    "    #example = torch.movedim(example,(0,1,2),(-1,-2,-3))\n",
    "    axarr[1].imshow(recon[0].detach().numpy())\n",
    "\n",
    "    break    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "0747f93ff6db21b2db2bf35ad4858dd0825b9c21797c41b4cc32097944ab3f10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
