{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bda6f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\pl_bolts\\models\\self_supervised\\amdim\\amdim_module.py:35: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\pl_bolts\\models\\self_supervised\\amdim\\amdim_module.py:93: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\pl_bolts\\losses\\self_supervised_learning.py:234: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n",
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\pl_bolts\\datamodules\\experience_source.py:18: UnderReviewWarning: The feature warn_missing_pkg is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  warn_missing_pkg(\"gym\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import datasets\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from pl_bolts.models.autoencoders.components import (\n",
    "    resnet18_decoder,\n",
    "    resnet18_encoder,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "tensor_transform = transforms.ToTensor()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a0f6381",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1439af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "def plot():\n",
    "    f, axarr = plt.subplots(2)\n",
    "\n",
    "    for i, item in enumerate(image):\n",
    "    # Reshape the array for plotting\n",
    "        item = item.reshape(-1, 28, 28)\n",
    "        axarr[0].imshow(item[0].cpu())\n",
    "\n",
    "    for i, item in enumerate(reconstructed):\n",
    "        item = item.reshape(-1, 28, 28).cpu()\n",
    "        item = item.detach().numpy()\n",
    "        axarr[1].imshow(item[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "def showExample():\n",
    "    for image, _ in example_loader:\n",
    "        f, axarr = plt.subplots(2)\n",
    "        image = image.reshape(-1,28*28).to(device)\n",
    "\n",
    "        model.to(device)\n",
    "        recon = model(image)\n",
    "\n",
    "        image = image.reshape(-1, 28, 28)\n",
    "        axarr[0].imshow(image[0].cpu())\n",
    "\n",
    "\n",
    "        recon = recon.reshape(-1, 28, 28).to('cpu')\n",
    "        axarr[1].imshow(recon[0].detach().numpy())\n",
    "\n",
    "        break\n",
    "\n",
    "def add_noise(inputs,variance):\n",
    "    noise = torch.randn_like(inputs)\n",
    "    return inputs + variance*noise\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b7f315",
   "metadata": {},
   "source": [
    "### KL Divergence intuition\n",
    "How we are minimizing KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab7d44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(self,z,mu,std):\n",
    "    p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "    q = torch.distributions.Normal(mu, std)\n",
    "    \n",
    "    log_pz = p.log_prob(z)\n",
    "    log_qzx = q.log_prob(z)\n",
    "    \n",
    "    kl = (log_qxz - log_pz)\n",
    "    \n",
    "    #sum over last dim to go from single dim distr. to multi-dim\n",
    "    kl = kl.sum(-1)\n",
    "    return kl\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1283e874",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28  #784\n",
    "hidden_size = 128\n",
    "code_size = 32\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Encoder \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size,hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size,code_size)\n",
    "        )\n",
    "        \n",
    "        #Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(code_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.model = nn.Sequential(\n",
    "        \n",
    "        )    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91435f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(pl.LightningModule):\n",
    "    def __init__(self, enc_out_dim=512, latent_dim=256, input_height=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.encoder = resnet18_encoder(False,False)\n",
    "        self.decoder = resnet18_decoder(latent_dim = latent_dim,\n",
    "                                        input_height = input_height,\n",
    "                                        first_conv=False,\n",
    "                                        maxpool1=False\n",
    "                                        \n",
    "                                        )\n",
    "        \n",
    "        self.fc_mu = nn.Linear(enc_out_dim,latent_dim)\n",
    "        self.fc_var = nn.Linear(enc_out_dim,latent_dim)\n",
    "        \n",
    "        #for gaussian distribution\n",
    "        self.log_scale = nn.Parameter(torch.Tensor([0.0]))\n",
    "        \n",
    "        def configure_optimizers(self):\n",
    "            return torch.optim.Adam(self.parameters(),lr=1e-4)\n",
    "        \n",
    "        \n",
    "        def gaussian_likelihood(self,x_hat,logscale,x):\n",
    "            scale = torch.exp(logscale)\n",
    "            mean = x_hat\n",
    "            dist = torch.distributions.Normal(mean,scale)\n",
    "            \n",
    "            \n",
    "            #prob of seing z under p(x|z)\n",
    "            log_pxz = dist.log_prob(x)\n",
    "            return log_pxz.sun(dim=(1,2,3))\n",
    "        \n",
    "        def kl_divergence(self,z,mu,std):\n",
    "            #define probabilities\n",
    "            p = torch.distributions.Normal(torch.zeros_like(mu),\n",
    "                                           torch.ones_like(std))\n",
    "            \n",
    "            #2. get probabilbities from equation\n",
    "            log_qzx = q.log_prob(z)\n",
    "            log_pz = p.log_prob(z)\n",
    "            \n",
    "            \n",
    "            #kl\n",
    "            kl = (log_qzx- log_pz)\n",
    "            kl = kl.sum(-1)\n",
    "            return kl\n",
    "        \n",
    "        \n",
    "        \n",
    "        def training_step(self,batch,batch_idx):\n",
    "            x, = batch\n",
    "            \n",
    "            #encode x to get mu and std\n",
    "            x_encoded = self.encoder(x)\n",
    "            mu,log_var = self.fc_mu(x_encoded),self.fc_var(x_encoded)\n",
    "            \n",
    "            #sample z from q\n",
    "            std = torch.exp(log_var / 2)\n",
    "            q = torch.distributions.Normal(mu,std)\n",
    "            z = q.rsample()\n",
    "            \n",
    "            \n",
    "            #decoded\n",
    "            x_hat = self.decoder(z)\n",
    "            \n",
    "            #reconstruction loss\n",
    "            recon_loss = self.gaussian_likelihood(x_hat,self.log_scale,x)\n",
    "            \n",
    "            #kl\n",
    "            kl = self.kl_divergence(z,mu,std)\n",
    "            \n",
    "            #elbo\n",
    "            elbo = (kl-recon_loss) \n",
    "            elbo = elbo.mean()\n",
    "            \n",
    "            self.log_dict({\n",
    "            'elbo': elbo,\n",
    "            'kl': kl.mean(),\n",
    "            'recon_loss': recon_loss.mean(),\n",
    "            'reconstruction': recon_loss.mean(),\n",
    "            'kl': kl.mean(),\n",
    "        })\n",
    "        \n",
    "            return elbo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87c0657",
   "metadata": {},
   "source": [
    "First distribution:  \n",
    "**q(z|x)**  \n",
    "We generate mu, std from the encoder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e678ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae  = VAE()\n",
    "\n",
    "#image from cifar-10(3 channels, 32*32)\n",
    "x = torch.rand(1,3,32,32)\n",
    "print('Image Shape {}'.format(x.shape))\n",
    "\n",
    "#Encode x to get mu and variance parameters\n",
    "x_encoded = vae.encoder(x)\n",
    "mu, log_var = vae.fc_mu(x_encoded),vae.fc_var(x_encoded)\n",
    "\n",
    "print('Encoded Image Shape {}'.format(x_encoded.shape))\n",
    "print('μ {}'.format(mu.shape))\n",
    "print('log_var {}'.format(log_var.shape))\n",
    "\n",
    "# SAMPLE Z from Q(Z|x)\n",
    "std = torch.exp(log_var / 2)\n",
    "q = torch.distributions.Normal(mu, std)\n",
    "z = q.rsample()\n",
    "\n",
    "print('z shape:', z.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33aa886",
   "metadata": {},
   "source": [
    "The second distribution:  \n",
    "**p(z)**   \n",
    "is the prior which we will fix to specific location(0,1)  \n",
    "By doing this, KL Willl force q(z|x) to move close to p\n",
    "\n",
    "At start, distributions will look like this:\n",
    "![](markdown/distributions.jpg)\n",
    "\n",
    "\n",
    "\n",
    "And over time, the q will mmove close to p, as it has learnable parameters\n",
    "\n",
    "![](markdown/distributions-over-time.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a3c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = torch.zeros_like(mu)\n",
    "one = torch.ones_like(std)\n",
    "p = torch.distributions.Normal(zero,one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c11be7f",
   "metadata": {},
   "source": [
    "The third distribution:  \n",
    "**p(x|z)**  \n",
    "(or reconstruction) will be used to measure the probability of seeing the image give that z was sampled\n",
    "\n",
    "Note: x_hat is **not** an image. They are parameters for a distribution  \n",
    "When used with MNIST dataset, they can form an image, but for colored images this isn't true "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55898553",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat = vae.decoder(z)\n",
    "print(x_hat.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3249cc",
   "metadata": {},
   "source": [
    "Lastly, we use x to parameterize a likehood distribution so that we can measure the probability of an input uder the (very highly dimensional) distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f0ec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_scale = nn.Parameter(torch.Tensor([0.0]))\n",
    "scale = torch.exp(log_scale)\n",
    "dist = torch.distributions.Normal(x_hat,scale)\n",
    "log_pxz = dist.log_prob(x)\n",
    "print(log_pxz.shape)\n",
    "\n",
    "\n",
    "#sum across all channels, pixels\n",
    "log_pxz = log_pxz.sum(dim=(1,2,3))\n",
    "print(log_pxz.shape)\n",
    "print('recon Loss {}'.format(log_pxz.item()))\n",
    "\n",
    "recon_loss = log_pxz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41849cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_bolts.datamodules import CIFAR10DataModule, ImagenetDataModule\n",
    "\n",
    "# you can use ImagenetDataModule if you want\n",
    "# imagenet = ImagenetDataModule('.')\n",
    "\n",
    "# for this tutorial we'll use cifar10\n",
    "cifar_10 = CIFAR10DataModule('.')\n",
    "\n",
    "pl.seed_everything(1234)\n",
    "\n",
    "vae = VAE()\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=30, progress_bar_refresh_rate=10)\n",
    "trainer.fit(vae, cifar_10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcf43f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.pyplot import imshow, figure\n",
    "import numpy as np\n",
    "from torchvision.utils import make_grid\n",
    "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization\n",
    "figure(figsize=(8, 3), dpi=300)\n",
    "\n",
    "# Z COMES FROM NORMAL(0, 1)\n",
    "num_preds = 16\n",
    "p = torch.distributions.Normal(torch.zeros_like(mu), torch.ones_like(std))\n",
    "z = p.rsample((num_preds,))\n",
    "\n",
    "# SAMPLE IMAGES\n",
    "with torch.no_grad():\n",
    "    pred = vae.decoder(z.to(vae.device)).cpu()\n",
    "\n",
    "# UNDO DATA NORMALIZATION\n",
    "normalize = cifar10_normalization()\n",
    "mean, std = np.array(normalize.mean), np.array(normalize.std)\n",
    "img = make_grid(pred).permute(1, 2, 0).numpy() * std + mean\n",
    "\n",
    "# PLOT IMAGES\n",
    "imshow(img);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
