{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3bda6f8f",
      "metadata": {
        "id": "3bda6f8f",
        "outputId": "29083446-177e-46c8-fcfb-93b484d7bb8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:cpu\n"
          ]
        }
      ],
      "source": [
        "# %% Import and stuff\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import torchvision.utils as vutils\n",
        "from  torch.utils import data\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "import matplotlib.colors as mcolors\n",
        "import os\n",
        "import gc\n",
        "\n",
        "\n",
        "NUM_EPOCHS = 60\n",
        "LR = 0.0008\n",
        "LATENT_SPACE_SIZE = 20\n",
        "IMG_SIZE = 28\n",
        "CHANNELS = 1\n",
        "B1 = 0.5\n",
        "B2 = 0.999\n",
        "RANDOM_SEED = 123\n",
        "\n",
        "\n",
        "STATE_DICT = \"state_dict\"\n",
        "MODEL_OPTIMIZER = \"model_optimizer\"\n",
        "LOSSES = \"losses\"\n",
        "RECON_LOSS = \"recon_loss\"\n",
        "KL_DIV = \"kl_div\"\n",
        "\n",
        "SHUFFLE = True\n",
        "PIN_MEMORY = True\n",
        "NUM_WORKERS = 0\n",
        "BATCH_SIZE = 2000\n",
        "\n",
        "specific_latent = torch.tensor([[0.7628, 0.1779, 0.3978, 0.3606, 0.6387,\n",
        "         0.3044, 0.8340, 0.3884, 0.9313, 0.5635, 0.1994, 0.6934, 0.5326,\n",
        "         0.3676, 0.5342, 0.9480, 0.4120, 0.5845, 0.4035, 0.5298, 0.0177,\n",
        "         0.5605, 0.6453, 0.9576, 0.7153, 0.1923, 0.8122, 0.0937, 0.5744,\n",
        "         0.5951, 0.8890, 0.4838, 0.5707, 0.6760, 0.3738, 0.2796, 0.1549,\n",
        "         0.8220, 0.2800, 0.4051, 0.2553, 0.1831, 0.0046, 0.9021, 0.0264,\n",
        "         0.2327, 0.8261, 0.0534, 0.1582, 0.4087, 0.9047, 0.1409, 0.6864,\n",
        "         0.1439, 0.3432, 0.1072, 0.5907, 0.6756, 0.6942, 0.6814, 0.3368,\n",
        "         0.4138, 0.8030, 0.7024, 0.3309, 0.7288, 0.2193, 0.1954, 0.9948,\n",
        "         0.1201, 0.9483, 0.7407, 0.4849, 0.6500, 0.8649, 0.7405, 0.4725,\n",
        "         0.5373, 0.6541, 0.5444, 0.7425, 0.8940, 0.3580, 0.3905, 0.8924,\n",
        "         0.2995, 0.3726, 0.5399, 0.3057, 0.3380, 0.8313, 0.1137, 0.0120,\n",
        "         0.7714, 0.2561, 0.2569, 0.2994, 0.7648, 0.2413, 0.6101\n",
        "        ]])\n",
        "\n",
        "\n",
        "img_shape = (CHANNELS, IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Device:{}'.format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6a0f6381",
      "metadata": {
        "id": "6a0f6381"
      },
      "outputs": [],
      "source": [
        " # %%helper functions\n",
        "\n",
        "def set_all_seeds(seed):\n",
        "    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    \n",
        "def set_deterministic():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "    torch.set_deterministic(True)\n",
        "\n",
        "def plot():\n",
        "    for i,(image, _) in example_loader:\n",
        "        f, axarr = plt.subplots(2)\n",
        "    \n",
        "        # Reshape the array for plotting\n",
        "        axarr[0].imshow(image[0].to(device))\n",
        "    \n",
        "\n",
        "        result = model.decoder(torch.tensor([-0.0,0.03]).to(device))\n",
        "        result = result.squeeze(0)\n",
        "        result = result.squeeze(0)\n",
        "        axarr[1].imshow(result[0].to('cpu').numpy())\n",
        "                  \n",
        "def add_noise(inputs,variance):\n",
        "    noise = torch.randn_like(inputs)\n",
        "    return inputs + variance*noise\n",
        "\n",
        "\n",
        "def save_checkpoint(state, filename):\n",
        "    print(\"=> Saving chekpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint):\n",
        "    model.load_state_dict(checkpoint[STATE_DICT])\n",
        "    optimizer.load_state_dict(checkpoint[MODEL_OPTIMIZER])\n",
        "    losses = checkpoint[LOSSES]\n",
        "    recon_losses = checkpoint[RECON_LOSS]\n",
        "    kl_losses = checkpoint[KL_DIV]\n",
        "    return losses, kl_losses, recon_losses\n",
        "    \n",
        "    \n",
        "def get_numbered_images():  \n",
        "    numberred_images = []\n",
        "    \n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        for i in range(len(labels[:])):\n",
        "            if batch_idx == labels[i].item():\n",
        "                numberred_images.append(images[i])\n",
        "                break          \n",
        "    \n",
        "    return numberred_images\n",
        "          \n",
        "\n",
        "def plot_generated_images(c, figsize=(20, 2.5), n_images=10):\n",
        "    model.to(device)\n",
        "    c = torch.tensor(c, dtype=torch.int64).to(device)\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=n_images, \n",
        "                             sharex=True, sharey=True, figsize=figsize)\n",
        "    \n",
        "    for batch_idx, (images, _) in enumerate(train_loader):\n",
        "        \n",
        "        images = images.to(device)\n",
        "        with torch.no_grad():\n",
        "           encoded, z_mean, z_log_var, decoded_images = model(images,c)[:n_images]\n",
        "\n",
        "        orig_images = images[:n_images]\n",
        "        break\n",
        "\n",
        "    for i in range(n_images):\n",
        "        for ax, img in zip(axes, [orig_images, decoded_images]):\n",
        "            curr_img = img[i].detach().to(torch.device('cpu'))\n",
        "            ax[i].imshow(curr_img.view((28, 28)))\n",
        "            \n",
        "            \n",
        "def plot_numberred_images(numbered_images,figsize=(20, 2.5)):\n",
        "\n",
        "    with torch.no_grad(): \n",
        "        fig, axes = plt.subplots(nrows=2, ncols=10, \n",
        "                                 sharex=True, sharey=True, figsize=(20, 2.5))\n",
        "\n",
        "        for i in range(10):\n",
        "            latent = torch.rand_like(torch.Tensor(20)).to(device)\n",
        "            c = torch.tensor(i, dtype=torch.int64).to(device) \n",
        "            \n",
        "            gen_img = model.decoder(latent,c).detach().to(torch.device('cpu'))\n",
        "            axes[0][i].imshow(numbered_images[i].view((28, 28))) \n",
        "            axes[1][i].imshow(gen_img.view((28, 28)))\n",
        "        \n",
        "        plt.savefig('varying_latent_space/latent_space'+str(LATENT_SPACE_SIZE)+'.png')\n",
        "        plt.figure().clear()\n",
        "            \n",
        "def plot_image(c):\n",
        "    \n",
        "\n",
        "    with torch.no_grad():\n",
        "        fig, axes = plt.subplots(nrows=1, ncols=1, sharex=True, sharey=True)\n",
        "        \n",
        "        c = torch.tensor(c, dtype=torch.int64).to(device)\n",
        "        latent = torch.rand_like(torch.Tensor(20)).to(device)\n",
        "    \n",
        "        decoded = model.decoder(latent,c).detach().to(torch.device('cpu'))\n",
        "        axes.imshow(decoded.view((28, 28)))   \n",
        "        \n",
        "#plot a grid of r,c images,reccomended with 10,10\n",
        "def plot_many_images(r=10,c=10):\n",
        "    fig, axes = plt.subplots(nrows=r, ncols=c,figsize=(20, 20), sharex=True, sharey=True)\n",
        "    \n",
        "    for i in range(r):\n",
        "        for y in range(c):\n",
        "            caption = torch.tensor(y, dtype=torch.int64).to(device)\n",
        "            latent = torch.rand_like(torch.Tensor(LATENT_SPACE_SIZE)).to(device)\n",
        "    \n",
        "            decoded = model.decoder(latent,caption).detach().to(torch.device('cpu'))\n",
        "            axes[i][y].imshow(decoded.view((28, 28)))   \n",
        "    \n",
        "    plt.savefig('varying_latent_space/latent_size'+str(LATENT_SPACE_SIZE)+'.png')\n",
        "    plt.figure().clear()\n",
        "\n",
        "\n",
        "def plot_latent_space_with_labels(iteration, num_classes=10):\n",
        "    d = {i:[] for i in range(num_classes)}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (features, targets) in enumerate(train_loader):\n",
        "\n",
        "            features = features.to(device)\n",
        "            targets = targets.to(device)\n",
        "            \n",
        "            embedding = model.encoding_fn(features)\n",
        "\n",
        "            for i in range(num_classes):\n",
        "                if i in targets:\n",
        "                    mask = targets == i\n",
        "                    d[i].append(embedding[mask].to('cpu').numpy())\n",
        "\n",
        "    colors = list(mcolors.TABLEAU_COLORS.items())\n",
        "    for i in range(num_classes):\n",
        "        d[i] = np.concatenate(d[i])\n",
        "        plt.scatter(\n",
        "            d[i][:, 0], d[i][:, 1],\n",
        "            #color=colors[i][1],\n",
        "            #label=f'{i}',\n",
        "            alpha=0.5)\n",
        "\n",
        "    #plt.legend()\n",
        "    #plt.savefig('latent_space/iteration'+str(iteration)+'.png')\n",
        "    plt.figure().clear()\n",
        "\n",
        "\n",
        "def plot_losses():\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.title(\"Loss During Training\")\n",
        "    plt.plot(losses[:], label=\"L\")\n",
        "    plt.plot(kl_losses[:], label=\"KL\")\n",
        "    plt.plot(recon_losses[:], label=\"Recon\")\n",
        "    plt.xlabel(\"iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    \n",
        "    plt.savefig('varying_plot_losses/latent_size'+str(LATENT_SPACE_SIZE)+'.png')\n",
        "    plt.figure().clear()\n",
        "    \n",
        "def clear_cache():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "fa1439af",
      "metadata": {
        "id": "fa1439af",
        "outputId": "ac57f11b-4565-4466-ac70-bd57a93d8e16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /root/torch_datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 91739931.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/torch_datasets/MNIST/raw/train-images-idx3-ubyte.gz to /root/torch_datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /root/torch_datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 27631317.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/torch_datasets/MNIST/raw/train-labels-idx1-ubyte.gz to /root/torch_datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /root/torch_datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 26872963.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/torch_datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to /root/torch_datasets/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /root/torch_datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4742476.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/torch_datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to /root/torch_datasets/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# %%Train Data\n",
        "\n",
        "set_deterministic\n",
        "set_all_seeds(RANDOM_SEED)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()])\n",
        "\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"~/torch_datasets\", train=True, transform=transform, download=True\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"~/torch_datasets\", train=False, transform=transform, download=True\n",
        ")\n",
        "\n",
        "train_loader = data.DataLoader(\n",
        "                                train_dataset,\n",
        "                                batch_size=BATCH_SIZE,\n",
        "                                shuffle=SHUFFLE,\n",
        "                                num_workers=NUM_WORKERS,\n",
        "                                pin_memory=PIN_MEMORY,\n",
        "                                drop_last=False\n",
        "                                )\n",
        "\n",
        "test_loader = data.DataLoader(\n",
        "                                test_dataset,\n",
        "                                batch_size=32,\n",
        "                                shuffle=True,\n",
        "                                num_workers=0\n",
        "                                )\n",
        "\n",
        "example_loader = data.DataLoader(\n",
        "                                train_dataset,\n",
        "                                batch_size=1,\n",
        "                                shuffle=True,\n",
        "                                num_workers=0,\n",
        "                                drop_last=True,\n",
        "                                )#Helper functions\n",
        "def plot():\n",
        "    f, axarr = plt.subplots(2)\n",
        "\n",
        "    for i, item in enumerate(image):\n",
        "    # Reshape the array for plotting\n",
        "        item = item.reshape(-1, 28, 28)\n",
        "        axarr[0].imshow(item[0].cpu())\n",
        "\n",
        "    for i, item in enumerate(reconstructed):\n",
        "        item = item.reshape(-1, 28, 28).cpu()\n",
        "        item = item.detach().numpy()\n",
        "        axarr[1].imshow(item[0])\n",
        "        \n",
        "        \n",
        "        \n",
        "def showExample():\n",
        "    for image, _ in example_loader:\n",
        "        f, axarr = plt.subplots(2)\n",
        "        image = image.reshape(-1,28*28).to(device)\n",
        "\n",
        "        model.to(device)\n",
        "        recon = model(image)\n",
        "\n",
        "        image = image.reshape(-1, 28, 28)\n",
        "        axarr[0].imshow(image[0].cpu())\n",
        "\n",
        "\n",
        "        recon = recon.reshape(-1, 28, 28).to('cpu')\n",
        "        axarr[1].imshow(recon[0].detach().numpy())\n",
        "\n",
        "        break\n",
        "\n",
        "def add_noise(inputs,variance):\n",
        "    noise = torch.randn_like(inputs)\n",
        "    return inputs + variance*noise\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1283e874",
      "metadata": {
        "id": "1283e874"
      },
      "outputs": [],
      "source": [
        "# %%Model\n",
        "\n",
        "class Reshape(nn.Module):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__()\n",
        "        self.shape = args\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.view(self.shape)\n",
        "\n",
        "\n",
        "class Trim(nn.Module):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, :, :28, :28]\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "           \n",
        "        self.z_mean = torch.nn.Linear(3136, LATENT_SPACE_SIZE) # 2 dim for visualization purposes\n",
        "        self.z_log_var = torch.nn.Linear(3136, LATENT_SPACE_SIZE)\n",
        "        \n",
        "        self.d_emb = nn.Embedding(10, 50)\n",
        "        self.e_emb_fc = nn.Linear(50, 784)\n",
        "        self.d_emb_fc = nn.Linear(50, 49)\n",
        "       \n",
        "        #########\n",
        "        # Encoder\n",
        "        #########\n",
        "        self.e_conv1 = nn.Conv2d(1, 32, stride=(1, 1), kernel_size=(3, 3), padding=1)\n",
        "        self.e_conv2 = nn.Conv2d(32, 64, stride=(2, 2), kernel_size=(3, 3), padding=1)\n",
        "        self.e_conv3 = nn.Conv2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1)\n",
        "        self.e_conv4 = nn.Conv2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1)\n",
        "        self.flatten = nn.Flatten()\n",
        "        \n",
        "\n",
        "        #########\n",
        "        # Decoder\n",
        "        #########  \n",
        "        self.d_lin = torch.nn.Linear(LATENT_SPACE_SIZE, 3087)\n",
        "        self.d_conv1 = nn.ConvTranspose2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1)\n",
        "        self.d_conv2 = nn.ConvTranspose2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1)              \n",
        "        self.d_conv3 = nn.ConvTranspose2d(64, 32, stride=(2, 2), kernel_size=(3, 3), padding=0)              \n",
        "        self.d_conv4 = nn.ConvTranspose2d(32, 1, stride=(1, 1), kernel_size=(3, 3), padding=0)\n",
        "       \n",
        "        \n",
        "    def encoder(self, x, c):\n",
        "        #c = self.e_emb(c)\n",
        "        #c = self.e_emb_fc(c)\n",
        "        #c = c.view(-1, 1, 28, 28)\n",
        "        #x = torch.cat((x, c), 1)\n",
        "\n",
        "        x = F.leaky_relu(self.e_conv1(x))\n",
        "        x = F.leaky_relu(self.e_conv2(x))\n",
        "        x = F.leaky_relu(self.e_conv3(x))\n",
        "        x = self.e_conv4(x)\n",
        "        x = self.flatten(x)\n",
        "        return x\n",
        "    \n",
        "    \n",
        "    def decoder(self, x, c):\n",
        "        c = self.d_emb(c)\n",
        "        c = self.d_emb_fc(c)\n",
        "        c = c.view(-1, 1, 7, 7)\n",
        "        \n",
        "        x = self.d_lin(x)\n",
        "        x = x.view(-1, 63, 7, 7)\n",
        "        \n",
        "        x = torch.cat((c, x), 1)\n",
        "        \n",
        "        x = x.view(-1,64,7,7)\n",
        "        x = F.leaky_relu(self.d_conv1(x))\n",
        "        x = F.leaky_relu(self.d_conv2(x))\n",
        "        x = F.leaky_relu(self.d_conv3(x))\n",
        "        x = F.leaky_relu(self.d_conv4(x))\n",
        "        x = x[:, :, :28, :28]\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n",
        "    \n",
        "    def encoding_fn(self, x, c):\n",
        "        x = self.encoder(x, c)\n",
        "        z_mean, z_log_var = self.z_mean(x), self.z_log_var(x)\n",
        "        encoded = self.reparameterize(z_mean, z_log_var)\n",
        "        return encoded\n",
        "        \n",
        "    def reparameterize(self, z_mu, z_log_var):\n",
        "        eps = torch.randn(z_mu.size(0), z_mu.size(1)).to(z_mu.get_device())\n",
        "        z = z_mu + eps * torch.exp(z_log_var/2.) \n",
        "        return z\n",
        "        \n",
        "    def forward(self, x, c):\n",
        "        x = self.encoder(x, c)\n",
        "        z_mean, z_log_var = self.z_mean(x), self.z_log_var(x)\n",
        "        encoded = self.reparameterize(z_mean, z_log_var) # sample μ,σ to create distribution\n",
        "        decoded = self.decoder(encoded,c)\n",
        "        return encoded, z_mean, z_log_var, decoded\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d91435f7",
      "metadata": {
        "id": "d91435f7"
      },
      "outputs": [],
      "source": [
        "# %% Loss func \n",
        "\n",
        "model = VAE()\n",
        " \n",
        "# Validation using MSE Loss function\n",
        "loss_function = nn.MSELoss(reduction='none')\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, betas=(B1 ,B2))\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    loss_function.cuda()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69e678ef",
      "metadata": {
        "id": "69e678ef"
      },
      "outputs": [],
      "source": [
        "# %%Train Model\n",
        "\n",
        "if 'numbered_images' not in locals():    \n",
        "   #numbered_images = get_numbered_images()\n",
        "   pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to(memory_format=torch.channels_last)\n",
        "    loss_function.to(device)\n",
        "    model.train()\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    \n",
        "\n",
        "\n",
        "logging_interval = 10\n",
        "losses = []\n",
        "kl_losses = []\n",
        "recon_losses = []\n",
        "recon_loss = 0\n",
        "kl_div = 0\n",
        "iter = 0\n",
        "alpha = 1\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    loss_function.cuda()\n",
        "    \n",
        "for epoch in range(4):\n",
        "    #st = time.time()\n",
        "    for batch, (imgs, labels) in enumerate(train_loader):\n",
        "\n",
        "        imgs = imgs.to(device,memory_format=torch.channels_last)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # set gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        encoded, z_mean, z_log_var, decoded = model(imgs,labels)\n",
        "\n",
        "        # Total Loss = Recon_loss + KLDivergence\n",
        "        kl_div = -0.5 * torch.sum(1 + z_log_var \n",
        "                                  - z_mean**2 \n",
        "                                  - torch.exp(z_log_var), \n",
        "                                  axis=1) # sum over latent dimension\n",
        "\n",
        "        kl_div = kl_div.mean()\n",
        "\n",
        "        recon_loss = loss_function(decoded, imgs)\n",
        "        recon_loss = recon_loss.view(BATCH_SIZE, -1).sum(axis=1) # sum over pixels\n",
        "        recon_loss = recon_loss.mean() # average over batch dimension\n",
        "        \n",
        "        loss = alpha*recon_loss + kl_div\n",
        "        loss.backward()\n",
        "        \n",
        "        \n",
        "        optimizer.step()  # Updates Weights\n",
        "        \n",
        "        loss_item = loss.item()\n",
        "        recon_loss_item = recon_loss.item()\n",
        "        kl_div_item = kl_div.item()\n",
        "        \n",
        "        #scheduler.step()\n",
        "        \n",
        "        \n",
        "        if iter % logging_interval == 0:\n",
        "            print('[%d/%d][%d/%d]\\t, LOSS:%.4f (recon_loss : %.4f, kl_loss = %.6f'\n",
        "                  %(epoch, NUM_EPOCHS, batch, len(train_loader), loss_item, recon_loss_item, kl_div_item))\n",
        "            #plot_numberred_images(iter*BATCH_SIZE,numbered_images)\n",
        "        \n",
        "        \n",
        "        losses.append(loss_item)\n",
        "        kl_losses.append(kl_div_item)\n",
        "        recon_losses.append(recon_loss_item)\n",
        "        \n",
        "        iter +=1\n",
        "\n",
        "#plot_many_images()     \n",
        "#plot_losses()\n",
        "#print('----------------------RESTARTING--------------------')\n",
        "#print(time.time()-st)\n",
        "    \n",
        "            \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "511a3c3e",
      "metadata": {
        "id": "511a3c3e"
      },
      "outputs": [],
      "source": [
        "# %% Save Model\n",
        "checkpoint = {STATE_DICT : model.state_dict(),\n",
        "              MODEL_OPTIMIZER : optimizer.state_dict(),\n",
        "              LOSSES: losses,\n",
        "              RECON_LOSS:recon_losses,\n",
        "              KL_DIV:kl_losses}\n",
        "save_checkpoint(checkpoint, \"cVAE4.pth.tar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "55898553",
      "metadata": {
        "id": "55898553"
      },
      "outputs": [],
      "source": [
        "# %%  Load Model\n",
        "losses,kl_losses,recon_losses = load_checkpoint(torch.load(\"cVAE4.pth.tar\",map_location=(device)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "b8f0ec35",
      "metadata": {
        "id": "b8f0ec35",
        "outputId": "7ad4c776-e82d-45b8-87a3-043fed9e79eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# %% print reconstructed image vs generated image\n",
        "if 'numbered_images' not in locals():\n",
        "    numbered_images = get_numbered_images()\n",
        "\n",
        "torch.device = device\n",
        "model = model.to(device)\n",
        "\n",
        "c = 4\n",
        "with torch.no_grad():\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=1, sharex=True, sharey=True)\n",
        "    #for batch_idx, (images, labels) in enumerate(example_loader):\n",
        "    \n",
        "    c = torch.tensor(c, dtype=torch.int64)#.to(device)\n",
        "    c1 = torch.tensor(4, dtype=torch.int64)#.to(device)\n",
        "    latent = torch.rand_like(torch.Tensor(20))#.to(device)\n",
        "\n",
        "    \n",
        "    decoded = model.decoder(latent,c).detach()#.to(device)\n",
        "    \n",
        "    #_, _, _, decoded2 = model(numbered_images[c.item()],c)\n",
        "    #enc = model.encoding_fn(numbered_images[c.item()][None, :],c1)\n",
        "    #enc = model.decoder(enc,c)\n",
        "    #enc = enc.detach().to(device)\n",
        "    \n",
        "    #image from dataset\n",
        "    axes[0].imshow(numbered_images[c.item()][None, :].view((28, 28)))   \n",
        "    \n",
        "    #reconstructed\n",
        "    #axes[1].imshow(decoder2.view((28, 28))) \n",
        "    \n",
        "    #image from decoder, caption\n",
        "    axes[1].imshow(decoded.view((28, 28)))   \n",
        "        "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}