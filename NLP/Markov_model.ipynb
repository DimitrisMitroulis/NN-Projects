{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d4a28888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5b64cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = len(AlPoe)\n",
    "\n",
    "AlanTrain, AlanTest  = AlPoe[:int(le*0.8)] , AlPoe[-int(le*0.2):]\n",
    "\n",
    "le = len(RobFrost)\n",
    "RobTrain, RobTest = RobFrost[:int(le*0.8)] , RobFrost[-int(le*0.2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "edecc0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/allan_poe.txt', 'r', encoding='utf-8') as file:\n",
    "    alan = file.read()\n",
    "\n",
    "with open('datasets/robert_frost.txt', 'r', encoding='utf-8') as file:\n",
    "    rob = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b7b88acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_txt(txt):\n",
    "    uniqueWords = {}\n",
    "    word2vec = []\n",
    "    vec2word = []\n",
    "    idx=0  \n",
    "    for index, word in enumerate(txt.split(\" \")):\n",
    "        if word not in uniqueWords:\n",
    "            uniqueWords[word] = idx\n",
    "            idx += 1\n",
    "        word2vec.append(uniqueWords.get(word))\n",
    "        vec2word.append(word)\n",
    "        \n",
    "    return uniqueWords, word2vec, vec2word\n",
    "    \n",
    "    \n",
    "    \n",
    "def UnknownIndex(df,table):\n",
    "    uniqueWords = []\n",
    "    # for each row\n",
    "    for index, row in df.iterrows():\n",
    "        row = re.sub(\" , [0-2][0-9]:[0-5][0-9]\", \"\",str(row.values))\n",
    "        row = re.sub(\"[-,|!|.|?|\\\"}\\][\\']\", \"\", row)\n",
    "        words = [w.lower() for w in row.split()]\n",
    "\n",
    "        for word in words:\n",
    "            if word not in uniqueWords and word not in table:\n",
    "                uniqueWords.append(word)\n",
    "                \n",
    "    \n",
    "    return uniqueWords\n",
    "\n",
    "def cleanup(item):\n",
    "    item = re.sub(\" , [0-2][0-9]:[0-5][0-9]\", \"\",item)\n",
    "    item = re.sub(\"[-()-,|!|.|?|\\\"{}\\][\\']\", \"\", item)\n",
    "    return item\n",
    "\n",
    "\n",
    "def normalize(txt):\n",
    "    clean_txt = \"\"\n",
    "    txt = re.sub(\"\\\\n\", \" \",txt)\n",
    "    txt = re.sub(\"\\\\'\", \" \",txt)\n",
    "    txt = txt.replace('\\u2009', '')\n",
    "    for index, word in enumerate(txt.split(\" \")):\n",
    "        word = re.sub(\" , [0-2][0-9]:[0-5][0-9]\", \"\",word)\n",
    "        word = word.translate(str.maketrans('', '', string.punctuation))\n",
    "        word = re.sub(\" {1,}\", \"\", word)\n",
    "        word = word.lower()\n",
    "        clean_txt+= word + \" \"\n",
    "    return clean_txt\n",
    "\n",
    "# tokenize sequence\n",
    "AlanuniqueWords = tokenize(AlanTrain)\n",
    "RobuniqueWords = tokenize(RobTrain)\n",
    "\n",
    "AlanUnknownWords = UnknownIndex(AlanTest, AlanTrain)\n",
    "RobUnknownWords = UnknownIndex(RobTest, RobTrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "1185789f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17378\n"
     ]
    }
   ],
   "source": [
    "# correct 1250\n",
    "\n",
    "\n",
    "\n",
    "Aunique, Aword2vector, Avector2word = tokenize_txt(alan)\n",
    "Runique, Rword2vector, Rvector2word = tokenize_txt(rob)\n",
    "\n",
    "input_text = Avector2word + Rvector2word\n",
    "\n",
    "labels = [0] * len(Avector2word)\n",
    "print(len(labels))\n",
    "labels += [1] * len(Rvector2word)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "unique = Runique.copy()\n",
    "unique.update(Aunique)\n",
    "\n",
    "vector2word = Avector2word+Rvector2word\n",
    "word2vector = Aword2vector+Rword2vector\n",
    "\n",
    "train_text, test_text, Ytrain, Ytest = train_test_split(vector2word, labels)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "bf873684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "204\n",
      "most\n"
     ]
    }
   ],
   "source": [
    "# should print true \n",
    "print(unique.get('most')) \n",
    "\n",
    "\n",
    "print(word2vector[1249])\n",
    "print(vector2word[1249])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "db490a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sequence(array, target):\n",
    "    count = 0\n",
    "    targ_len = len(target)\n",
    "    \n",
    "    for i in range(len(array) - targ_len+1):\n",
    "        if array[i:i+targ_len] == target:\n",
    "            count+=1\n",
    "    \n",
    "    return count\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def markov_model(sequence, order=2,eps=1):\n",
    "    #tokenize sequence\n",
    "    unique,word2vec,vec2word = tokenize_txt(sequence)\n",
    "    \n",
    "    lenght = len(vec2word)\n",
    "    A = np.empty((lenght, lenght))\n",
    "    \n",
    "    markov_model = {}  \n",
    "    \n",
    "    #loop from start to end-order\n",
    "    for i, word in enumerate(vec2word[:-order]):\n",
    "        con = []\n",
    "        \n",
    "        # add next words to context\n",
    "        context = vec2word[i:i+order+1]\n",
    "\n",
    "        next_char = vec2word[i+order]\n",
    "    \n",
    "        \n",
    "    \n",
    "    return markov_model\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def predict_seq(markov_model, initial_context, length):\n",
    "    current_context = initial_context\n",
    "    generated_text = current_context\n",
    "    for _ in range(length):\n",
    "        if current_context not in markov_model:\n",
    "            break\n",
    "        next_chars = list(markov_model[current_context].keys())\n",
    "        next_char = random.choice(next_chars)\n",
    "        generated_text += next_char\n",
    "        current_context = generated_text[-len(initial_context):]\n",
    "    return generated_text\n",
    "\n",
    "    unique = tokenize_txt(sequence)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return null\n",
    "    \n",
    "    #start from start+count\n",
    "    for i, word in enumerate(sequence.split(\" \")[count:]):\n",
    "        break\n",
    "        context = []    \n",
    "        \n",
    "        \n",
    "        for index in range(count):\n",
    "            context.append(index+i)\n",
    "        \n",
    "        for j, word2 in enumerate(sequence[count:]):\n",
    "            #num =  \n",
    "            #denom = c\n",
    "            #A[i][j] = \n",
    "            pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "3e6fac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize A\n",
    "V = len(unique)\n",
    "\n",
    "A0 = np.ones((V,V))\n",
    "pi0 = np.ones(V)\n",
    "\n",
    "\n",
    "A1 = np.ones((V,V))\n",
    "pi1 = np.ones(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94236e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_counts(text_as_int, A,pi):\n",
    "    for tokens in text_as_int:\n",
    "        las_idx = None\n",
    "        for idx in tokens:\n",
    "            if last_idx is None:\n",
    "                # it's the first word in a sentence\n",
    "                pi[idx]+=1\n",
    "            else:\n",
    "                # the last word exists, so count a transition\n",
    "                A[last_idx, idx] +=1\n",
    "            #update last idex\n",
    "            last_idx = idx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
